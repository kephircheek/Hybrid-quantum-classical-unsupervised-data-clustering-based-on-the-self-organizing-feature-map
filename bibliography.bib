% Encoding: UTF-8

@ARTICLE{lecun2015,
AUTHOR    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey}, 
TITLE     = {Deep learning},
JOURNAL   = {Nature},
YEAR      = {2015},
Volume    = {521},
Number    = {7553},
Pages     = {436-444},
Url       = {https://doi.org/10.1038/nature14539},
Abstract  = { Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
Note      = {},
Key       = {}
},

@MISC{kaggle2014,
AUTHOR    = {}, 
TITLE     = {Kaggle. Higgs boson machine learning challenge.},
JOURNAL   = {https://www.kaggle.com/c/higgs-boson (2014)},
YEAR      = {2014},
Volume    = {},
Number    = {},
Pages     = {},
Url       = {},
Abstract  = {},
Note      = {},
Key       = {}
},

@ARTICLE{esteva2019,
AUTHOR    = {Esteva, Andre 
    and Robicquet, Alexandre 
    and Ramsundar, Bharath 
    and Kuleshov, Volodymyr 
    and DePristo, Mark 
    and Chou, Katherine 
    and Cui, Claire 
    and Corrado, Greg 
    and Thrun, Sebastian 
    and Dean, Jeff}, 
TITLE     = {A guide to deep learning in healthcare},
JOURNAL   = {Nature Medicine},
YEAR      = {2019},
Volume    = {25},
Number    = {1},
Pages     = {24-29},
Url       = {https://doi.org/10.1038/s41591-018-0316-z},
Abstract  = {Here we present deep-learning techniques for healthcare, centering our discussion on deep learning in computer vision, natural language processing, reinforcement learning, and generalized methods. We describe how these computational techniques can impact a few key areas of medicine and explore how to build end-to-end systems. Our discussion of computer vision focuses largely on medical imaging, and we describe the application of natural language processing to domains such as electronic health record data. Similarly, reinforcement learning is discussed in the context of robotic-assisted surgery, and generalized deep-learning methods for genomics are reviewed.},
Note      = {},
Key       = {}
},

@ARTICLE{kohonen1990, 
author={T. {Kohonen}}, 
journal={Proceedings of the IEEE}, 
title={The self-organizing map}, 
year={1990}, 
volume={78}, 
number={9}, 
pages={1464-1480}, 
abstract={The self-organized map, an architecture suggested for artificial neural networks, is explained by presenting simulation experiments and practical applications. The self-organizing map has the property of effectively creating spatially organized internal representations of various features of input signals and their abstractions. One result of this is that the self-organization process can discover semantic relationships in sentences. Brain maps, semantic maps, and early work on competitive learning are reviewed. The self-organizing map algorithm (an algorithm which order responses spatially) is reviewed, focusing on best matching cell selection and adaptation of the weight vectors. Suggestions for applying the self-organizing map algorithm, demonstrations of the ordering process, and an example of hierarchical clustering of data are presented. Fine tuning the map by learning vector quantization is addressed. The use of self-organized maps in practical speech recognition and a simulation experiment on semantic mapping are discussed.<>}, 
keywords={learning systems;neural nets;self-adjusting systems;speech recognition;self-organizing map;neural networks;semantic maps;competitive learning;clustering;learning vector;speech recognition;Biological neural networks;Artificial neural networks;Pattern recognition;Process control;Signal processing;Computer networks;Signal processing algorithms;Animals;Organizing;Speech recognition}, 
doi={10.1109/5.58325}, 
ISSN={1558-2256}, 
month={Sep.},
},

@Article{radovic2018,
author={Radovic, Alexander
and Williams, Mike
and Rousseau, David
and Kagan, Michael
and Bonacorsi, Daniele
and Himmel, Alexander
and Aurisano, Adam
and Terao, Kazuhiro
and Wongjirad, Taritree},
title={Machine learning at the energy and intensity frontiers of particle physics},
journal={Nature},
year={2018},
volume={560},
number={7716},
pages={41-48},
abstract={Our knowledge of the fundamental particles of nature and their interactions is summarized by the standard model of particle physics. Advancing our understanding in this field has required experiments that operate at ever higher energies and intensities, which produce extremely large and information-rich data samples. The use of machine-learning techniques is revolutionizing how we interpret these data samples, greatly increasing the discovery potential of present and future experiments. Here we summarize the challenges and opportunities that come with the use of machine learning at the frontiers of particle physics.},
issn={1476-4687},
doi={10.1038/s41586-018-0361-2},
url={https://doi.org/10.1038/s41586-018-0361-2}
},

@Article{lloyd2013,
  author  = {Seth Lloyd and Masoud Mohseni and Patrick Rebentrost},
  title   = {Quantum algorithms for supervised and unsupervised machine learning},
  year = {2013},
  journal = {arXiv: 1307.0411},
}
,

@article{rebentrost2018,
  title = {Quantum Hopfield neural network},
  author = {Rebentrost, Patrick and Bromley, Thomas R. and Weedbrook, Christian and Lloyd, Seth},
  journal = {Phys. Rev. A},
  volume = {98},
  issue = {4},
  pages = {042308},
  numpages = {11},
  year = {2018},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.98.042308},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.98.042308}
},

@Article{vilibic2016,
author={Vilibic, Ivica
and Sepic, Jadranka
and Mihanovic, Hrvoje
and Kalinic, Hrvoje
and Cosoli, Simone
and Janekovic, Ivica
and Zagar, Nedjeljka
and Jesenko, Blaz
and Tudor, Martina
and Dadic, Vlado
and Ivankovic, Damir},
title={Self-Organizing Maps-based ocean currents forecasting system},
journal={Scientific Reports},
year={2016},
volume={6},
number={1},
pages={22924},
abstract={An ocean surface currents forecasting system, based on a Self-Organizing Maps (SOM) neural network algorithm, high-frequency (HF) ocean radar measurements and numerical weather prediction (NWP) products, has been developed for a coastal area of the northern Adriatic and compared with operational ROMS-derived surface currents. The two systems differ significantly in architecture and algorithms, being based on either unsupervised learning techniques or ocean physics. To compare performance of the two methods, their forecasting skills were tested on independent datasets. The SOM-based forecasting system has a slightly better forecasting skill, especially during strong wind conditions, with potential for further improvement when data sets of higher quality and longer duration are used for training.},
issn={2045-2322},
doi={10.1038/srep22924},
url={https://doi.org/10.1038/srep22924}
},

@ARTICLE{tyrsa2017,
AUTHOR    = {Tyrsa, Vera
    and Carrio, Adrian
    and Sampedro, Carlos
    and Rodriguez-Ramos, Alejandro
    and Campoy, Pascual}, 
TITLE     = {A Review of Deep Learning Methods and Applications for Unmanned Aerial Vehicles},
JOURNAL   = {Journal of Sensors},
YEAR      = {2017},
Volume    = {2017},
Number    = {},
Pages     = {},
Url       = {https://doi.org/10.1155/2017/3296874},
Abstract  = {Deep learning is recently showing outstanding results for solving a wide variety of robotic tasks in the areas of perception, planning, localization, and control. Its excellent capabilities for learning representations from the complex data acquired in real environments make it extremely suitable for many kinds of autonomous robotic applications. In parallel, Unmanned Aerial Vehicles (UAVs) are currently being extensively applied for several types of civilian tasks in applications going from security, surveillance, and disaster rescue to parcel delivery or warehouse management. In this paper, a thorough review has been performed on recent reported uses and applications of deep learning for UAVs, including the most relevant developments as well as their performances and limitations. In addition, a detailed explanation of the main deep learning techniques is provided. We conclude with a description of the main challenges for the application of deep learning for UAV-based solutions.},
Note      = {},
Key       = {}
},

@Article{butler2018,
author={Butler, Keith T.
and Davies, Daniel W.
and Cartwright, Hugh
and Isayev, Olexandr
and Walsh, Aron},
title={Machine learning for molecular and materials science},
journal={Nature},
year={2018},
volume={559},
number={7715},
pages={547-555},
abstract={Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence.},
issn={1476-4687},
doi={10.1038/s41586-018-0337-2},
url={https://doi.org/10.1038/s41586-018-0337-2}
},


@Article{ghahramani2015,
author={Ghahramani, Zoubin},
title={Probabilistic machine learning and artificial intelligence},
journal={Nature},
year={2015},
volume={521},
number={7553},
pages={452-459},
abstract={How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
issn={1476-4687},
doi={10.1038/nature14541},
url={https://doi.org/10.1038/nature14541}
},

@Article{jeswal2019,
author={Jeswal, S. K.
and Chakraverty, S.},
title={Recent Developments and Applications in Quantum Neural Network: A Review},
journal={Archives of Computational Methods in Engineering},
year={2019},
volume={26},
number={4},
pages={793-807},
abstract={Quantum neural network is a useful tool which has seen more development over the years mainly after twentieth century. Like artificial neural network (ANN), a novel, useful and applicable concept has been proposed recently which is known as quantum neural network (QNN). QNN has been developed combining the basics of ANN with quantum computation paradigm which is superior than the traditional ANN. QNN is being used in computer games, function approximation, handling big data etc. Algorithms of QNN are also used in modelling social networks, associative memory devices, and automated control systems etc. Different models of QNN has been proposed by different researchers throughout the world but systematic study of these models have not been done till date. Moreover, application of QNN may also be seen in some of the related research papers. As such, this paper includes different models which have been developed and further the implement of the same in various applications. In order to understand the powerfulness of QNN, few results and reasons are incorporated to show that these new models are more useful and efficient than traditional ANN.},
issn={1886-1784},
doi={10.1007/s11831-018-9269-0},
url={https://doi.org/10.1007/s11831-018-9269-0}
},


@article{dunjko2016,
  title = {Quantum-Enhanced Machine Learning},
  author = {Dunjko, Vedran and Taylor, Jacob M. and Briegel, Hans J.},
  journal = {Phys. Rev. Lett.},
  volume = {117},
  issue = {13},
  pages = {130501},
  numpages = {6},
  year = {2016},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.117.130501},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.117.130501},
  abstract = {The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised, and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.}
},

@article{paparo2014,
  title = {Quantum Speedup for Active Learning Agents},
  author = {Paparo, Giuseppe Davide and Dunjko, Vedran and Makmal, Adi and Martin-Delgado, Miguel Angel and Briegel, Hans J.},
  journal = {Phys. Rev. X},
  volume = {4},
  issue = {3},
  pages = {031002},
  numpages = {14},
  year = {2014},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.4.031002},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.4.031002},
  abstract = {Can quantum mechanics help us build intelligent learning agents? A defining signature of intelligent behavior is the capacity to learn from experience. However, a major bottleneck for agents to learn in real-life situations is the size and complexity of the corresponding task environment. Even in a moderately realistic environment, it may simply take too long to rationally respond to a given situation. If the environment is impatient, allowing only a certain time for a response, an agent may then be unable to cope with the situation and to learn at all. Here, we show that quantum physics can help and provide a quadratic speedup for active learning as a genuine problem of artificial intelligence. This result will be particularly relevant for applications involving complex task environments.}
},


@ARTICLE{doszkocs1990,
AUTHOR    = {Doszkocs, T. E and  Reggia, J and Lin, X},
TITLE     = {Connectionist models and information retrieval. Annual Review of Information Science and Technology},
JOURNAL   = {ARIST},
YEAR      = {1990},
Volume    = {25},
Number    = {},
Pages     = {209-260.},
Url       = {},
Abstract  = {},
Note      = {},
Key       = {}
},


@ARTICLE{key,
AUTHOR    = {}, 
TITLE     = {},
JOURNAL   = {},
YEAR      = {},
Volume    = {},
Number    = {},
Pages     = {},
Url       = {},
Abstract  = {},
Note      = {},
Key       = {}
},

@book{guido1998,
    author    = {Deboeck, Guido and  Kohonen, Teuvo},
    title     = {Visual Explorations in Finance},
    year      = {1998},
    publisher = {Springer-Verlag London},
    address   = {Reading, Massachusetts},
    doi = {10.1007/978-1-4471-3913-3},
},

@article{qml0,
	author = {{Asselmeyer-Maluga, Torsten}},
	title = {Quantum computing and the brain: quantum nets, dessins d\'{}enfants and neural networks},
	DOI= "10.1051/epjconf/201919800014",
	url= "https://doi.org/10.1051/epjconf/201919800014",
	journal = {EPJ Web Conf.},
	year = 2019,
	volume = 198,
	pages = "14",
	abstract = "In this paper, we will discuss a formal link between neural networks and quantum computing. For that purpose we will present a simple model for the description of the neural network by forming sub-graphs of the whole network with the same or a similar state. We will describe the interaction between these areas by closed loops, the feedback loops. The change of the graph is given by the deformations of the loops. This fact can be mathematically formalized by the fundamental group of the graph. Furthermore the neuron has two basic states |0〉 (ground state) and |1〉 (excited state). The whole state of an area of neurons is the linear combination of the two basic state with complex coefficients representing the signals (with 3 Parameters: amplitude, frequency and phase) along the neurons. If something changed in this area, we need a transformation which will preserve this general form of a state (mathematically, this transformation must be an element of the group S L(2; C)). The same argumentation must be true for the feedback loops, i.e. a general transformation of states along the feedback loops is an assignment of this loop to an element of the transformation group. Then it can be shown that the set of all signals forms a manifold (character variety) and all properties of the network must be encoded in this manifold. In the paper, we will discuss how to interpret learning and intuition in this model. Using the Morgan-Shalen compactification, the limit for signals with large amplitude can be analyzed by using quasi-Fuchsian groups as represented by dessins d’enfants (graphs to analyze Riemannian surfaces). As shown by Planat and collaborators, these dessins d’enfants are a direct bridge to (topological) quantum computing with permutation groups. The normalization of the signal reduces to the group S U(2) and the whole model to a quantum network. Then we have a direct connection to quantum circuits. This network can be transformed into operations on tensor networks. Formally we will obtain a link between machine learning and Quantum computing."
},

@INPROCEEDINGS{qml1, 
author={A. {Daskin}}, 
booktitle={2018 IEEE International Conference on Systems, Man, and Cybernetics}, 
title={A Simple Quantum Neural Net with a Periodic Activation Function}, 
year={2018}, 
volume={}, 
number={}, 
pages={2887-2891}, 
keywords={backpropagation;computational complexity;gradient methods;neural nets;quantum gates;quantum gates;cosine values;gradient descent;machine learning problems;equivalent classical neural net;quantum circuit;input parameters;qubits;periodic activation function;simple quantum neural net;Qubit;Logic gates;Biological neural networks;Machine learning;Computational modeling;quantum machine learning;quantum neural networks}, 
doi={10.1109/SMC.2018.00491}, 
ISSN={1062-922X}, 
month={Oct},
abstract = {In this paper, we propose a simple neural net that requires only O(n log_2k) number of qubits and O(nk) quantum gates: Here, N is the number of input parameters, and k is the number of weights applied to these parameters in the proposed neural net. We describe the network in terms of a quantum circuit, and then draw its equivalent classical neural net which involves O(k^n) nodes in the hidden layer. Then, we show that the network uses a periodic activation function of cosine values of the linear combinations of the inputs and weights. The backpropagation is described through the gradient descent, and then iris and breast cancer datasets are used for the simulations. The numerical results indicate the network can be used in machine learning problems and it may provide exponential speedup over the same structured classical neural net.}},


@article{qml2,
   title={Image Classification Using Quantum Inference on the D-Wave 2X},
   ISBN={9781538691700},
   url={http://dx.doi.org/10.1109/ICRC.2018.8638596},
   OPTdoi={10.1109/icrc.2018.8638596},
   journal={2018 IEEE International Conference on Rebooting Computing},
   publisher={IEEE},
   author={Nguyen, Nga T.T. and Kenyon, Garrett T.},
   year={2018},
   month={Nov},
   abstract= {We use a quantum annealing D-Wave 2X computer to obtain solutions to NP-hard sparse coding problems. To reduce the dimensionality of the sparse coding problem to fit on the quantum D-Wave 2X hardware, we passed downsampled MNIST images through a bottleneck autoencoder. To establish a benchmark for classification performance on this reduced dimensional data set, we used an AlexNet-like architecture implemented in TensorFlow, obtaining a classification score of 94.54±0.7\%. As a control, we showed that the same AlexNet-like architecture produced near-state-of-the-art classification performance (∼99\%) on the original MNIST images. To obtain a set of optimized features for inferring sparse representations of the reduced dimensional MNIST dataset, we imprinted on a random set of 47 image patches followed by an off-line unsupervised learning algorithm using stochastic gradient descent to optimize for sparse coding. Our single-layer of sparse coding matched the stride and patch size of the first convolutional layer of the AlexNet-like deep neural network and contained 47 fully-connected features, 47 being the maximum number of dictionary elements that could be embedded onto the D-Wave 2X hardware. Recent work suggests that the optimal level of sparsity corresponds to a critical value of the trade-off parameter associated with a putative second order phase transition, an observation supported by a free energy analysis of D-Wave energy states. When the sparse representations inferred by the D-Wave 2X were passed to a linear support vector machine, we obtained a classification score of 95.68\%. Thus, on this problem, we find that a single-layer of quantum inference is able to outperform a standard deep neural network architecture.}
}


@Article{med2,
author={Ghasemi, Sorayya},
title={Cancer's epigenetic drugs: where are they in the cancer medicines?},
journal={The Pharmacogenomics Journal},
year={2019},
abstract={Epigenetic modulation can affect the characteristics of cancers. Because it is likely to manipulate epigenetic genes, they can be considered as potential targets for cancer treatment. In this comprehensive study, epigenetic drugs are categorized according to anticancer mechanisms and phase of therapy. The relevant articles or databases were searched for epigenetic approaches to cancer therapy. Epigenetic drugs are divided according to their mechanisms and clinical phases that have been approved by the FDA or are undergoing evaluation phases. DNA methylation agents, chromatin remodelers specially HDACs, and noncoding RNAs especially microRNAs are the main epi-drugs for cancer. Despite many challenges, combination therapy using epi-drugs and routine therapies such as chemotherapy in various approaches have exhibited beneficial effects compared with each treatment alone. Cancer stem cell targeting and epigenetic editing have been confirmed as definitive pathways for cancer treatment. This paper reviewed the available epigenetic approaches to cancer therapy.},
pages={1473-1150},
OPTdoi={10.1038/s41397-019-0138-5},
url={https://doi.org/10.1038/s41397-019-0138-5}
},

@Article{med1,
author={Steck, Susan E.
and Murphy, E. Angela},
title={Dietary patterns and cancer risk},
journal={Nature Reviews Cancer},
year={2019},
abstract={Over the past decade, the search for dietary factors on which to base cancer prevention guidelines has led to the rapid expansion of the field of dietary patterns and cancer. Multiple systematic reviews and meta-analyses have reported epidemiological associations between specific cancer types and both data-driven dietary patterns determined by empirical analyses and investigator-defined dietary indexes based on a predetermined set of dietary components. New developments, such as the use of metabolomics to identify objective biomarkers of dietary patterns and novel statistical techniques, could provide further insights into the links between diet and cancer risk. Although animal models of dietary patterns are limited, progress in this area could identify the potential mechanisms underlying the disease-specific associations observed in epidemiological studies. In this Review, we summarize the current state of the field, provide a critical appraisal of new developments and identify priority areas for future research. An underlying theme that emerges is that the effectiveness of different dietary pattern recommendations in reducing risk could depend on the type of cancer or on other risk factors such as family history, sex, age and other lifestyle factors or comorbidities as well as on metabolomic signatures or gut microbiota profiles.},
issn={1474-1768},
OPTdoi={10.1038/s41568-019-0227-4},
url={https://doi.org/10.1038/s41568-019-0227-4}
},

@Article{med0,
author={Cheng, Yuan
and He, Cai
and Wang, Manni
and Ma, Xuelei
and Mo, Fei
and Yang, Shengyong
and Han, Junhong
and Wei, Xiawei},
title={Targeting epigenetic regulators for cancer therapy: mechanisms and advances in clinical trials},
journal={Signal Transduction and Targeted Therapy},
year={2019},
volume={4},
number={1},
pages={62},
abstract={Epigenetic alternations concern heritable yet reversible changes in histone or DNA modifications that regulate gene activity beyond the underlying sequence. Epigenetic dysregulation is often linked to human disease, notably cancer. With the development of various drugs targeting epigenetic regulators, epigenetic-targeted therapy has been applied in the treatment of hematological malignancies and has exhibited viable therapeutic potential for solid tumors in preclinical and clinical trials. In this review, we summarize the aberrant functions of enzymes in DNA methylation, histone acetylation and histone methylation during tumor progression and highlight the development of inhibitors of or drugs targeted at epigenetic enzymes.},
issn={2059-3635},
doi={10.1038/s41392-019-0095-0},
url={https://doi.org/10.1038/s41392-019-0095-0}
},


@article{bio2,
author = {Jiao, Lin and Chen, Jie and Wu, Xiaojuan and Cai, Bei and Su, Zhenzhen and Wang, Lanlan},
title = {Correlation of CpG methylation of the Pdcd1 gene with PD-1 expression on CD8+ T cells and medical laboratory indicators in chronic hepatitis B infection},
journal = {The Journal of Gene Medicine},
year={2019},
volume = {22},
number = {},
pages = {e3148},
keywords = {chronic hepatitis B infection, methylation, Pdcd1, programmed death-1},
OPTdoi = {10.1002/jgm.3148},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jgm.3148},
abstract = {Abstract Background The negative signal provided by some co-inhibitory factors such as programmed cell death-1 (PD-1) has been associated with chronic hepatitis B (CHB) infection induced-T cell exhaustion, although the correlation of CpG methylation of the Pdcd1 gene with PD-1 expression and medical laboratory indicators in CHB infection has not yet been elucidated. Methods Blood samples from 20 CHB infection patients and 20 spontaneous clearance (SC) patients were collected. Percentages of PD-1-positive CD8+ T cells were analyzed by flow cytometry. The percentage of CpG methylation at the Pdcd1 locus was analyzed by bisulfite sequencing. Student's t test, Pearson and Spearman's correlation, and Mann–Whitney tests were used in the statistical analysis. Results Percentages of PD-1-positive CD8+ T cells in peripheral blood T cells were significantly higher in CHB patients than in the SC group (p < 0.001). The methylation level of Pdcd1 was significantly lower in CHB patients (p < 0.001) and the methylation level of Pdcd1 was negatively correlated with PD-1 expression level in CD8+ T cells (p < 0.001) and hepatitis-B surface antigen (HBsAg) (p < 0.001). Conclusions The results of the present study suggest that Pdcd1 methylation is correlated with PD-1 expression on CD8+ T cells and correlated with HBsAg and alanine aminotransferase. The results may provide new ideas regarding anti-PD-1 inhibitors, and epigenetic regulators such as demethylation inhibitors could represent more successful therapeutic strategies in hepatitis B infection patients.}
},

@article{bio1,
title = "Eukaryotic elongation factor 2 is involved in the anticoccidial action of diclazuril in the second-generation merozoites of Eimeria tenella",
journal = "Veterinary Parasitology",
volume = "276",
pages = "108991",
year = "2019",
issn = "0304-4017",
doi = "https://doi.org/10.1016/j.vetpar.2019.108991",
url = "http://www.sciencedirect.com/science/article/pii/S0304401719302729",
author = "Bian-hua Zhou and Liu-shu Jia and Hong-wei Guo and Hai-yan Ding and Jing-yun Yang and Hong-wei Wang",
keywords = ", Diclazuril, Eukaryotic elongation factor 2, Second-generation merozoites",
abstract = "Eimeria tenella, an obligate intracellular parasite, can actively invade the cecal epithelial cells of chickens and cause severe enteric disease. Eukaryotic elongation factor 2 (eEF2) plays a major role in protein synthesis and cell survival. This study aims to explore the exact mechanisms underlying diclazuril inhibition in second-generation merozoites of E. tenella. The eEF2 cDNA of the second-generation merozoites of E. tenella (EtEF2) was cloned by reverse transcriptase polymerase chain reaction and rapid amplification of cDNA ends. Diclazuril-induced expression profiles of EtEF2 were also analyzed. The cloned full-length cDNA (2893 bp) of the EtEF2 nucleotide sequence encompassed a 2499 bp open reading frame (ORF) that encoded a polypeptide of 832 residues with an estimated molecular mass of 93.12 kDa and a theoretical isoelectric point of 5.99. The EtEF2 nucleotide sequence was submitted to the GenBank database with the accession number KF188423. The EtEF2 protein sequence shared 99 \% homology with the eEF2 sequence of Toxoplasma gondii (GenBank XP_002367778.1). The GTPase activity domain and ADP-ribosylation domain were conserved signature sequences of the eEF2 gene family. The changes in the transcriptional and translational levels of EtEF2 were detected through quantitative real-time PCR and Western blot analyses. The mRNA expression level of EtEF2 was 2.706 fold increases and the protein level of EtEF2 was increased 67.31 \% under diclazuril treatment. In addition, the localization of EtEF2 was investigated through immunofluorescence assay. Experimental results demonstrated that EtEF2 was distributed primarily in the cytoplasm of second-generation merozoites, and its fluorescence intensity was enhanced after diclazuril treatment. These findings indicated that EtEF2 may have an important role in understanding the signaling mechanism underlying the anticoccidial action of diclazuril and could be a promising target for novel drug exploration."
},


@article{bio0,
title = "Circular RNA TTN Acts As a miR-432 Sponge to Facilitate Proliferation and Differentiation of Myoblasts via the IGF2/PI3K/AKT Signaling Pathway",
journal = "Molecular Therapy - Nucleic Acids",
volume = "18",
pages = "966 - 980",
year = "2019",
issn = "2162-2531",
doi = "https://doi.org/10.1016/j.omtn.2019.10.019",
url = "http://www.sciencedirect.com/science/article/pii/S2162253119303294",
author = "Xiaogang Wang and Xiukai Cao and Dong Dong and Xuemei Shen and Jie Cheng and Rui Jiang and Zhaoxin Yang and Shujun Peng and Yongzhen Huang and Xianyong Lan and Ibrahim Elsaeid Elnour and Chuzhao Lei and Hong Chen",
keywords = "bovine, circRNA, miRNA,  pathway, myoblast",
abstract = "Circular RNAs (circRNAs) are ubiquitous endogenous RNA found in various organisms that can regulate gene expression in eukaryotes. However, little is known about potential roles for circRNAs in muscle development. We analyzed circRNA sequencing data of bovine skeletal muscle tissue and found differential expression of circTitin (circTTN) in fetal and adult bovine muscle tissue. We then further studied the role of circTTN in bovine muscle development. Overexpression and inhibition of circTTN together elicited its promoting roles in proliferation and differentiation of bovine primary myoblasts. Mechanistically, circTTN showed interaction with miR-432 by luciferase screening and RNA immunoprecipitation (RIP) assays. Additionally, miR-432 is a regulator of insulin-like growth factor 2 (IGF2), as indicated by luciferase activity, quantitative real-time PCR, and western blotting assays. Increased miR-432 expression inhibited the expression of IGF2, but this effect was remitted by circTTN. Conclusively, our results showed that circTTN promoted proliferation and differentiation of bovine primary myoblasts via competitively combining with miR-432 to activate the IGF2/phosphatidylinositol 3-kinase (PI3K)/AKT signaling pathway."
}
@InProceedings{kohonen1997,
  author    = {T. Kohonen},
  year      = {1997},
  title     = {Exploration of very large databases by self-organizing maps},
  booktitle = {Proceedings of International Conference on Neural Networks},
  publisher = {IEEE},
  doi       = {10.1109/icnn.1997.611622},
}

@Article{vesanto2000,
  author    = {J. Vesanto and E. Alhoniemi},
  title     = {Clustering of the self-organizing map},
  journal   = {{IEEE} Transactions on Neural Networks},
  year      = {2000},
  volume    = {11},
  number    = {3},
  pages     = {586--600},
  month     = {may},
  doi       = {10.1109/72.846731},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{kohonen1996,
  author    = {T. Kohonen and E. Oja and O. Simula and A. Visa and J. Kangas},
  title     = {Engineering applications of the self-organizing map},
  journal   = {Proceedings of the {IEEE}},
  year      = {1996},
  volume    = {84},
  number    = {10},
  pages     = {1358--1384},
  doi       = {10.1109/5.537105},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{kiviluotoa1996,
  author    = {K. Kiviluoto},
  year      = {1996},
  title     = {Topology preservation in self-organizing maps},
  booktitle = {Proceedings of International Conference on Neural Networks},
  publisher = {IEEE},
  OPTdoi       = {10.1109/icnn.1996.548907},
}

@Article{jones2012,
  author    = {Felicity C. Jones and and Manfred G. Grabherr and Yingguang Frank Chan and Pamela Russell and Evan Mauceli and Jeremy Johnson and Ross Swofford and Mono Pirun and Michael C. Zody and Simon White and Ewan Birney and Stephen Searle and Jeremy Schmutz and Jane Grimwood and Mark C. Dickson and Richard M. Myers and Craig T. Miller and Brian R. Summers and Anne K. Knecht and Shannon D. Brady and Haili Zhang and Alex A. Pollen and Timothy Howes and Chris Amemiya and Eric S. Lander and Federica Di Palma and Kerstin Lindblad-Toh and David M. Kingsley},
  title     = {The genomic basis of adaptive evolution in threespine sticklebacks},
  journal   = {Nature},
  year      = {2012},
  volume    = {484},
  number    = {7392},
  pages     = {55--61},
  month     = {apr},
  doi       = {10.1038/nature10944},
  publisher = {Springer Science and Business Media {LLC}},
},

@Article{mori2019,
  author    = {
    Tomoya Mori 
    and Haruka Takaoka 
    and Junko Yamane 
    and Cantas Alev 
    and Wataru Fujibuchi
},
  title     = {Novel computational model of gastrula morphogenesis to identify spatial discriminator genes by self-organizing map (SOM) clustering},
  journal   = {Scientific Reports},
  year      = {2019},
  volume    = {9},
  number = {12597},
  pages    = {},
  month     = {aug},
  OPTdoi       = {10.1038/s41598-019-49031-1},
  publisher = {Springer Science and Business Media {LLC}},
},

@Article{corsello2017,
  author    = {Steven M Corsello and Joshua A Bittker and Zihan Liu and Joshua Gould and Patrick McCarren and Jodi E Hirschman and Stephen E Johnston and Anita Vrcic and Bang Wong and Mariya Khan and Jacob Asiedu and Rajiv Narayan and Christopher C Mader and Aravind Subramanian and Todd R Golub},
  title     = {The Drug Repurposing Hub: a next-generation drug library and information resource},
  journal   = {Nature Medicine},
  year      = {2017},
  volume    = {23},
  number    = {4},
  pages     = {405--408},
  month     = {apr},
  doi       = {10.1038/nm.4306},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{zhu2018,
  author    = {Daqi Zhu and Xiang Cao and Bing Sun and Chaomin Luo},
  title     = {Biologically Inspired Self-Organizing Map Applied to Task Assignment and Path Planning of an {AUV} System},
  journal   = {{IEEE} Transactions on Cognitive and Developmental Systems},
  year      = {2018},
  volume    = {10},
  number    = {2},
  pages     = {304--313},
  month     = {jun},
  doi       = {10.1109/tcds.2017.2727678},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{chea2016,
  author    = {Ratha Chea and Gaël Grenouillet and Sovan Lek},
  title     = {Evidence of Water Quality Degradation in Lower Mekong Basin Revealed by Self-Organizing Map},
  journal   = {{PLOS} {ONE}},
  year      = {2016},
  volume    = {11},
  number    = {1},
  OPTpages     = {e0145527},
  month     = {jan},
  OPTdoi       = {10.1371/journal.pone.0145527},
  editor    = {Chon-Lin Lee},
  publisher = {Public Library of Science ({PLoS})},
}

@Book{schwab2017,
  title     = {The Fourth Industrial Revolution},
  publisher = {Currency Press},
  year      = {2017},
  author    = {Schwab, Klaus},
  address   = {Redfern, New South Wales},
}

@Article{biamonte2017,
  author    = {Jacob Biamonte and Peter Wittek and Nicola Pancotti and Patrick Rebentrost and Nathan Wiebe and Seth Lloyd},
  title     = {Quantum machine learning},
  journal   = {Nature},
  year      = {2017},
  volume    = {549},
  number    = {7671},
  pages     = {195--202},
  month     = {sep},
  doi       = {10.1038/nature23474},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{schuld2014,
  author    = {Maria Schuld and Ilya Sinayskiy and Francesco Petruccione},
  title     = {An introduction to quantum machine learning},
  journal   = {Contemporary Physics},
  year      = {2014},
  volume    = {56},
  number    = {2},
  pages     = {172--185},
  month     = {oct},
  doi       = {10.1080/00107514.2014.964942},
  publisher = {Informa {UK} Limited},
}

@Article{killoran2019,
  author    = {Nathan Killoran and Thomas R. Bromley and Juan Miguel Arrazola and Maria Schuld and Nicol{\'{a}}s Quesada and Seth Lloyd},
  title     = {Continuous-variable quantum neural networks},
  journal   = {Physical Review Research},
  year      = {2019},
  volume    = {1},
  number    = {3},
  month     = {oct},
  OPTdoi       = {10.1103/physrevresearch.1.033063},
  publisher = {American Physical Society ({APS})},
}

@Article{purushothaman1997,
  author    = {G. Purushothaman and N.B. Karayiannis},
  title     = {Quantum neural networks ({QNNs}): inherently fuzzy feedforward neural networks},
  journal   = {{IEEE} Transactions on Neural Networks},
  year      = {1997},
  volume    = {8},
  number    = {3},
  pages     = {679--693},
  month     = {may},
  doi       = {10.1109/72.572106},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{schuld2014b,
  author    = {Maria Schuld and Ilya Sinayskiy and Francesco Petruccione},
  title     = {The quest for a Quantum Neural Network},
  journal   = {Quantum Information Processing},
  year      = {2014},
  volume    = {13},
  number    = {11},
  pages     = {2567--2586},
  month     = {aug},
  doi       = {10.1007/s11128-014-0809-8},
  publisher = {Springer Science and Business Media {LLC}},
}

% arxiv 2019

@Article{verdon2019,
  author  = {Guillaume Verdon and Trevor McCourt and Enxhell Luzhnica and Vikash Singh and Stefan Leichenauer and Jack Hidary},
  year = {2019},
  title   = {Quantum Graph Neural Networks},
  journal = {arXiv: 1909.12264},
}

@Article{bondarenko2019,
  author  = {Dmytro Bondarenko and Polina Feldmann},
  title   = {Quantum autoencoders to denoise quantum data},
  year = {2019},
  journal = {arXiv: 1910.09169},
}

@Article{mishra2019,
  author  = {Nilima Mishra and Aradh Bisarya and Shubham Kumar and Bikash K. Behera and Sabyasachi Mukhopadhyay and Prasanta K. Panigrahi},
  title   = {Cancer Detection Using Quantum Neural Networks: A Demonstration on a Quantum Computer},
  journal = {arXiv: 1911.00504},
  year = {2019},
}

@Article{liu2019,
  author  = {Junhua Liu and Kwan Hui Lim and Kristin L. Wood and Wei Huang and Chu Guo and He-Liang Huang},
  title   = {Hybrid Quantum-Classical Convolutional Neural Networks},
  journal = {arXiv: 1911.02998},
  year = {2019},
}

@Article{vinci2019,
  author  = {Walter Vinci and Lorenzo Buffoni and Hossein Sadeghi and Amir Khoshaman and Evgeny Andriyash and Mohammad H. Amin},
  title   = {A Path Towards Quantum Advantage in Training Deep Generative Models with Quantum Annealers},
  journal = {arXiv: 1912.02119},
  year = {2019},
}

@Article{tacchino2019,
  author  = {Francesco Tacchino and Panagiotis Barkoutsos and Chiara Macchiavello and Ivano Tavernelli and Dario Gerace and Daniele Bajoni},
  title   = {Quantum implementation of an artificial feed-forward neural network},
  journal = {arXiv: 1912.12486},
  year = {2019},
}

@Article{lu2019,
  author  = {Sirui Lu and Lu-Ming Duan and Dong-Ling Deng},
  title   = {Quantum Adversarial Machine Learning},
  journal = {arXiv: 2001.00030},
  year = {2019},
}

@Misc{ibmq,
  title = {https://www.ibm.com/quantum-computing/},
  url   = {https://www.ibm.com/quantum-computing/},
}

@Article{trugenberger2001,
  author    = {C. A. Trugenberger},
  title     = {Probabilistic Quantum Memories},
  journal   = {Physical Review Letters},
  year      = {2001},
  volume    = {87},
  number    = {6},
  month     = {jul},
  doi       = {10.1103/physrevlett.87.067901},
  publisher = {American Physical Society ({APS})},
}

@Article{cherny2019,
  author    = {Valentin V. Cherny and Tim Byrnes and Alexey N. Pyrkov},
  title     = {Nontrivial Attractors of the Perturbed Nonlinear Schrödinger Equation: Applications to Associative Memory and Pattern Recognition},
  journal   = {Advanced Quantum Technologies},
  year      = {2019},
  volume    = {2},
  number    = {7-8},
  pages     = {1800087},
  month     = {feb},
  doi       = {10.1002/qute.201800087},
  publisher = {Wiley},
}

@Article{byrnes2013,
  author    = {Tim Byrnes and Shinsuke Koyama and Kai Yan and Yoshihisa Yamamoto},
  title     = {Neural networks using two-component Bose-Einstein condensates},
  journal   = {Scientific Reports},
  year      = {2013},
  volume    = {3},
  number    = {1},
  month     = {aug},
  doi       = {10.1038/srep02531},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{sze2017,
  author    = {Vivienne Sze and Yu-Hsin Chen and Tien-Ju Yang and Joel S. Emer},
  title     = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  journal   = {Proceedings of the {IEEE}},
  year      = {2017},
  volume    = {105},
  number    = {12},
  pages     = {2295--2329},
  month     = {dec},
  doi       = {10.1109/jproc.2017.2761740},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{marcus2018,
  author  = {Marcus, Gary},
  title   = {Deep Learning: A Critical Appraisal},
  journal = {arxiv: 1801.00631},
  year    = {2018},
}

@Article{kourtis2020,
  author  = {Kornilios Kourtis and Martino Dazzi and Nikolas Ioannou and Tobias Grosser and Abu Sebastian and Evangelos Eleftheriou},
  title   = {Compiling Neural Networks for a Computational Memory Accelerator},
  journal = {arxiv: 2003.04293},
  year    = {2020},
}

@Article{dunjko2018,
  author    = {Vedran Dunjko and Hans J Briegel},
  title     = {Machine learning {\&} artificial intelligence in the quantum domain: a review of recent progress},
  journal   = {Reports on Progress in Physics},
  year      = {2018},
  volume    = {81},
  number    = {7},
  pages     = {074001},
  month     = {jun},
  doi       = {10.1088/1361-6633/aab406},
  publisher = {{IOP} Publishing},
}

@Article{carleo2019,
  author    = {Giuseppe Carleo and Ignacio Cirac and Kyle Cranmer and Laurent Daudet and Maria Schuld and Naftali Tishby and Leslie Vogt-Maranto and Lenka Zdeborov{\'{a}}},
  title     = {Machine learning and the physical sciences},
  journal   = {Reviews of Modern Physics},
  year      = {2019},
  volume    = {91},
  number    = {4},
  month     = {dec},
  doi       = {10.1103/revmodphys.91.045002},
  publisher = {American Physical Society ({APS})},
}

@Article{wiebe2012,
  author    = {Nathan Wiebe and Daniel Braun and Seth Lloyd},
  title     = {Quantum Algorithm for Data Fitting},
  journal   = {Physical Review Letters},
  year      = {2012},
  volume    = {109},
  number    = {5},
  month     = {aug},
  doi       = {10.1103/physrevlett.109.050505},
  publisher = {American Physical Society ({APS})},
}

@Article{harrow2009,
  author    = {Aram W. Harrow and Avinatan Hassidim and Seth Lloyd},
  title     = {Quantum Algorithm for Linear Systems of Equations},
  journal   = {Physical Review Letters},
  year      = {2009},
  volume    = {103},
  number    = {15},
  month     = {oct},
  doi       = {10.1103/physrevlett.103.150502},
  publisher = {American Physical Society ({APS})},
}

@Article{childs2017,
  author    = {Andrew M. Childs and Robin Kothari and Rolando D. Somma},
  title     = {Quantum Algorithm for Systems of Linear Equations with Exponentially Improved Dependence on Precision},
  journal   = {{SIAM} Journal on Computing},
  year      = {2017},
  volume    = {46},
  number    = {6},
  pages     = {1920--1950},
  month     = {jan},
  doi       = {10.1137/16m1087072},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Article{lloyd2014,
  author    = {Seth Lloyd and Masoud Mohseni and Patrick Rebentrost},
  title     = {Quantum principal component analysis},
  journal   = {Nature Physics},
  year      = {2014},
  volume    = {10},
  number    = {9},
  pages     = {631--633},
  month     = {jul},
  doi       = {10.1038/nphys3029},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{rebentrost2014,
  author    = {Patrick Rebentrost and Masoud Mohseni and Seth Lloyd},
  title     = {Quantum Support Vector Machine for Big Data Classification},
  journal   = {Physical Review Letters},
  year      = {2014},
  volume    = {113},
  number    = {13},
  month     = {sep},
  doi       = {10.1103/physrevlett.113.130503},
  publisher = {American Physical Society ({APS})},
}

@InCollection{kamruzzaman2019,
  author    = {Abu Kamruzzaman and Yousef Alhwaiti and Avery Leider and Charles C. Tappert},
  title     = {Quantum Deep Learning Neural Networks},
  booktitle = {Lecture Notes in Networks and Systems},
  publisher = {Springer International Publishing},
  year      = {2019},
  pages     = {299--311},
  month     = {feb},
  doi       = {10.1007/978-3-030-12385-7_24},
}

@Article{lewenstein1994,
  author    = {M. Lewenstein},
  title     = {Quantum Perceptrons},
  journal   = {Journal of Modern Optics},
  year      = {1994},
  volume    = {41},
  number    = {12},
  pages     = {2491--2501},
  month     = {dec},
  doi       = {10.1080/09500349414552331},
  publisher = {Informa {UK} Limited},
}

@Article{allcock2018,
  author  = {{Allcock}, Jonathan and {Hsieh}, Chang-Yu and {Kerenidis}, Iordanis and {Zhang}, Shengyu},
  title   = {Quantum algorithms for feedforward neural networks},
  journal = {arXiv:1812.03089},
  year    = {2018},
}

@Article{zhao2019,
  author    = {Zhikuan Zhao and Alejandro Pozas-Kerstjens and Patrick Rebentrost and Peter Wittek},
  title     = {Bayesian deep learning on a quantum computer},
  journal   = {Quantum Machine Intelligence},
  year      = {2019},
  volume    = {1},
  number    = {1-2},
  pages     = {41--51},
  month     = {may},
  doi       = {10.1007/s42484-019-00004-7},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{cong2019,
  author    = {Iris Cong and Soonwon Choi and Mikhail D. Lukin},
  title     = {Quantum convolutional neural networks},
  journal   = {Nature Physics},
  year      = {2019},
  volume    = {15},
  number    = {12},
  pages     = {1273--1278},
  month     = {aug},
  doi       = {10.1038/s41567-019-0648-8},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{kerenidis2018,
  author  = {{Kerenidis}, Iordanis and {Luongo}, Alessandro},
  title   = {Quantum classification of the MNIST dataset via Slow Feature Analysis},
  journal = {arXiv:1805.08837},
  year    = {2018},
}

@Article{li2019,
  author  = {{Li}, Tongyang and {Chakrabarti}, Shouvanik and {Wu}, Xiaodi},
  title   = {Sublinear quantum algorithms for training linear and kernel-based classifiers},
  journal = {arXiv:1904.02276},
  year    = {2019},
}

@Article{foesel2018,
  author    = {Thomas Fösel and Petru Tighineanu and Talitha Weiss and Florian Marquardt},
  title     = {Reinforcement Learning with Neural Networks for Quantum Feedback},
  journal   = {Physical Review X},
  year      = {2018},
  volume    = {8},
  number    = {3},
  month     = {sep},
  doi       = {10.1103/physrevx.8.031084},
  publisher = {American Physical Society ({APS})},
}

@Article{solan2001,
  author    = {Zach Solan and Eytan Ruppin},
  title     = {Similarity in Perception: A Window to Brain Organization},
  journal   = {Journal of Cognitive Neuroscience},
  year      = {2001},
  volume    = {13},
  number    = {1},
  pages     = {18--30},
  month     = {jan},
  doi       = {10.1162/089892901564144},
  publisher = {{MIT} Press - Journals},
}

@InProceedings{dunjko2017,
  author    = {Vedran Dunjko and Jacob M. Taylor and Hans J. Briegel},
  title     = {Advances in quantum reinforcement learning},
  booktitle = {2017 {IEEE} International Conference on Systems, Man, and Cybernetics ({SMC})},
  year      = {2017},
  month     = {oct},
  publisher = {{IEEE}},
  doi       = {10.1109/smc.2017.8122616},
}

@Article{nautrup2019,
  author    = {Hendrik Poulsen Nautrup and Nicolas Delfosse and Vedran Dunjko and Hans J. Briegel and Nicolai Friis},
  title     = {Optimizing Quantum Error Correction Codes with Reinforcement Learning},
  journal   = {Quantum},
  year      = {2019},
  volume    = {3},
  pages     = {215},
  month     = {dec},
  doi       = {10.22331/q-2019-12-16-215},
  publisher = {Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
}

@Comment{jabref-meta: databaseType:bibtex;}
