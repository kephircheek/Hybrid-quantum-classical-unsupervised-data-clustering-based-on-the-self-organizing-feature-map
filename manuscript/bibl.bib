
@misc{NobelPrizePhysics,
	title = {The {Nobel} {Prize} in {Physics} 2024},
	url = {https://www.nobelprize.org/prizes/physics/2024/press-release/},
	abstract = {The Nobel Prize in Physics 2024 was awarded jointly to John J. Hopfield and Geoffrey E. Hinton "for foundational discoveries and inventions that enable machine learning with artificial neural networks"},
	language = {en-US},
	urldate = {2024-12-04},
	journal = {NobelPrize.org},
	file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\584WQJ4Q\\press-release.html:text/html},
}


@article{zardiniQuantumKnearestNeighbors2024,
	title = {A quantum k-nearest neighbors algorithm based on the {Euclidean} distance estimation},
	volume = {6},
	issn = {2524-4914},
	url = {https://doi.org/10.1007/s42484-024-00155-2},
	doi = {10.1007/s42484-024-00155-2},
	abstract = {The k-nearest neighbors (k-NN) is a basic machine learning (ML) algorithm, and several quantum versions of it, employing different distance metrics, have been presented in the last few years. Although the Euclidean distance is one of the most widely used distance metrics in ML, it has not received much consideration in the development of these quantum variants. In this article, a novel quantum k-NN algorithm based on the Euclidean distance is introduced. Specifically, the algorithm is characterized by a quantum encoding requiring a low number of qubits and a simple quantum circuit not involving oracles, aspects that favor its realization. In addition to the mathematical formulation and some complexity observations, a detailed empirical evaluation with simulations is presented. In particular, the results have shown the correctness of the formulation, a drop in the performance of the algorithm when the number of measurements is limited, the competitiveness with respect to some classical baseline methods in the ideal case, and the possibility of improving the performance by increasing the number of measurements.},
	language = {en},
	number = {1},
	urldate = {2024-12-08},
	journal = {Quantum Machine Intelligence},
	author = {Zardini, Enrico and Blanzieri, Enrico and Pastorello, Davide},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Euclidean distance, k-Nearest neighbors, Quantum computing, Quantum Computing, Quantum machine learning},
	pages = {23},
	file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\7QRP79ZS\\Zardini и др. - 2024 - A quantum k-nearest neighbors algorithm based on the Euclidean distance estimation.pdf:application/pdf},
}

@article{rathQuantumDataEncoding2024,
  title = {Quantum Data Encoding: A Comparative Analysis of Classical-to-Quantum Mapping Techniques and Their Impact on Machine Learning Accuracy},
  shorttitle = {Quantum Data Encoding},
  author = {Rath, Minati and Date, Hema},
  year = {2024},
  month = dec,
  journal = {EPJ Quantum Technology},
  volume = {11},
  number = {1},
  pages = {1--22},
  publisher = {SpringerOpen},
  issn = {2196-0763},
  doi = {10.1140/epjqt/s40507-024-00285-3},
  urldate = {2024-12-08},
  abstract = {This study explores the integration of quantum data embedding techniques into classical machine learning (ML) algorithms; to assess performance enhancements and computational implications across a spectrum of models. We explored various classical-to-quantum mapping methods; ranging from basis encoding and angle encoding to amplitude encoding; for encoding classical data. We conducted an extensive empirical study encompassing popular ML algorithms, including Logistic Regression, K-Nearest Neighbors, Support Vector Machines, and ensemble methods like Random Forest, LightGBM, AdaBoost, and CatBoost. Our findings reveal that quantum data embedding contributes to improved classification accuracy and F1 scores, particularly notable in models that inherently benefit from enhanced feature representation. We observed nuanced effects on running time, with low-complexity models exhibiting moderate increases and more computationally intensive models experiencing discernible changes. Notably, ensemble methods demonstrated a favorable balance between performance gains and computational overhead. This study underscores the potential of quantum data embedding to enhance classical ML models and emphasizes the importance of weighing performance improvements against computational costs. Future research may involve refining quantum encoding processes to optimize computational efficiency and explore scalability for real-world applications. Our work contributes to the growing body of knowledge on the intersection of quantum computing and classical machine learning, offering insights for researchers and practitioners seeking to harness the advantages of quantum-inspired techniques in practical scenarios.},
  copyright = {2024 The Author(s)},
  langid = {english},
  file = {C:\Users\alex\Zotero\storage\525XATVX\Rath и Date - 2024 - Quantum data encoding a comparative analysis of classical-to-quantum mapping techniques and their i.pdf}
}

@article{yuQuantumAlgorithmsSimilarity2020,
	title = {Quantum {Algorithms} for {Similarity} {Measurement} {Based} on {Euclidean} {Distance}},
	volume = {59},
	issn = {1572-9575},
	url = {https://doi.org/10.1007/s10773-020-04567-1},
	doi = {10.1007/s10773-020-04567-1},
	abstract = {Similarity measurement is a fundamental problem that arise both on its own and as a key subroutine in more complex tasks, such as machine learning. However, in classical algorithms, the time used to similarity measurement usually increases exponentially as the amount of data and the number of data dimensions increase. In this paper, we presented three quantum algorithms based on Euclidean distance to measure the similarity between data sets. In the proposed algorithms, some special unitary operations are utilized to construct imperative quantum states from quantum random access memory. Then, a badly needed result for estimating the similarity between data sets, can be got by performing projective measurements. Furthermore, it is shown that these algorithms can achieve the exponential acceleration of the classical algorithm in the quantity or the dimension of data.},
	language = {en},
	number = {10},
	urldate = {2024-12-08},
	journal = {International Journal of Theoretical Physics},
	author = {Yu, Kai and Guo, Gong-De and Li, Jing and Lin, Song},
	month = oct,
	year = {2020},
	keywords = {Euclidean distance, Quantum algorithm, Quantum Computing, Quantum machine learning, Similarity measurement},
	pages = {3134--3144},
}



@misc{NobelPrizeChemistry,
	title = {The {Nobel} {Prize} in {Chemistry} 2024},
	url = {https://www.nobelprize.org/prizes/chemistry/2024/press-release/},
	abstract = {The Nobel Prize in Chemistry 2024 was divided, one half awarded to David Baker "for computational protein design", the other half jointly to Demis Hassabis and John Jumper "for protein structure prediction"},
	language = {en-US},
	urldate = {2024-12-04},
	journal = {NobelPrize.org},
	file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\5F96XNJF\\press-release.html:text/html},
}

@article{caroGeneralizationQuantumMachine2022,
	title = {Generalization in quantum machine learning from few training data},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-32550-3},
	doi = {10.1038/s41467-022-32550-3},
	abstract = {Modern quantum machine learning (QML) methods involve variationally optimizing a parameterized quantum circuit on a training data set, and subsequently making predictions on a testing data set (i.e., generalizing). In this work, we provide a comprehensive study of generalization performance in QML after training on a limited number N of training data points. We show that the generalization error of a quantum machine learning model with T trainable gates scales at worst as \$\${\textbackslash}sqrt\{T/N\}\$\$. When only K ≪ T gates have undergone substantial change in the optimization process, we prove that the generalization error improves to \$\${\textbackslash}sqrt\{K/N\}\$\$. Our results imply that the compiling of unitaries into a polynomial number of native gates, a crucial application for the quantum computing industry that typically uses exponential-size training data, can be sped up significantly. We also show that classification of quantum states across a phase transition with a quantum convolutional neural network requires only a very small training data set. Other potential applications include learning quantum error correcting codes or quantum dynamical simulation. Our work injects new hope into the field of QML, as good generalization is guaranteed from few training data.},
	language = {en},
	number = {1},
	urldate = {2024-12-04},
	journal = {Nature Communications},
	author = {Caro, Matthias C. and Huang, Hsin-Yuan and Cerezo, M. and Sharma, Kunal and Sornborger, Andrew and Cincio, Lukasz and Coles, Patrick J.},
	month = aug,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Quantum information},
	pages = {4919},
	file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\DBH9II24\\Caro и др. - 2022 - Generalization in quantum machine learning from few training data.pdf:application/pdf},
}

@article{anschuetzQuantumVariationalAlgorithms2022,
	title = {Quantum variational algorithms are swamped with traps},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-35364-5},
	doi = {10.1038/s41467-022-35364-5},
	abstract = {One of the most important properties of classical neural networks is how surprisingly trainable they are, though their training algorithms typically rely on optimizing complicated, nonconvex loss functions. Previous results have shown that unlike the case in classical neural networks, variational quantum models are often not trainable. The most studied phenomenon is the onset of barren plateaus in the training landscape of these quantum models, typically when the models are very deep. This focus on barren plateaus has made the phenomenon almost synonymous with the trainability of quantum models. Here, we show that barren plateaus are only a part of the story. We prove that a wide class of variational quantum models—which are shallow, and exhibit no barren plateaus—have only a superpolynomially small fraction of local minima within any constant energy from the global minimum, rendering these models untrainable if no good initial guess of the optimal parameters is known. We also study the trainability of variational quantum algorithms from a statistical query framework, and show that noisy optimization of a wide variety of quantum models is impossible with a sub-exponential number of queries. Finally, we numerically confirm our results on a variety of problem instances. Though we exclude a wide variety of quantum algorithms here, we give reason for optimism for certain classes of variational algorithms and discuss potential ways forward in showing the practical utility of such algorithms.},
	language = {en},
	number = {1},
	urldate = {2024-12-04},
	journal = {Nature Communications},
	author = {Anschuetz, Eric R. and Kiani, Bobak T.},
	month = dec,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Quantum information, Theoretical physics},
	pages = {7760},
	file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\QKJB7CYS\\Anschuetz и Kiani - 2022 - Quantum variational algorithms are swamped with traps.pdf:application/pdf},
}

@article{abbasPowerQuantumNeural2021,
	title = {The power of quantum neural networks},
	volume = {1},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-021-00084-1},
	doi = {10.1038/s43588-021-00084-1},
	abstract = {It is unknown whether near-term quantum computers are advantageous for machine learning tasks. In this work we address this question by trying to understand how powerful and trainable quantum machine learning models are in relation to popular classical neural networks. We propose the effective dimension—a measure that captures these qualities—and prove that it can be used to assess any statistical model’s ability to generalize on new data. Crucially, the effective dimension is a data-dependent measure that depends on the Fisher information, which allows us to gauge the ability of a model to train. We demonstrate numerically that a class of quantum neural networks is able to achieve a considerably better effective dimension than comparable feedforward networks and train faster, suggesting an advantage for quantum machine learning, which we verify on real quantum hardware.},
	language = {en},
	number = {6},
	urldate = {2024-12-04},
	journal = {Nature Computational Science},
	author = {Abbas, Amira and Sutter, David and Zoufal, Christa and Lucchi, Aurelien and Figalli, Alessio and Woerner, Stefan},
	month = jun,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Quantum information},
	pages = {403--409},
	file = {Отправленная версия:C\:\\Users\\alex\\Zotero\\storage\\GFTY8FWM\\Abbas и др. - 2021 - The power of quantum neural networks.pdf:application/pdf},
}

@misc{IBMRoadmapQuantumcentric,
	title = {{IBM} roadmap to quantum-centric supercomputers ({Updated} 2024) {\textbar} {IBM} {Quantum} {Computing} {Blog}},
	url = {https://www.ibm.com/quantum/blog/ibm-quantum-roadmap-2025},
	abstract = {The updated IBM Quantum roadmap: weaving quantum processors, CPUs, and GPUs into a compute fabric to solve problems beyond the scope of classical resources.},
	language = {en},
	urldate = {2024-12-04},
	file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\T3IIVYAY\\ibm-quantum-roadmap-2025.html:text/html},
}

@misc{acharyaQuantumErrorCorrection2024,
	title = {Quantum error correction below the surface code threshold},
	url = {http://arxiv.org/abs/2408.13687},
	doi = {10.48550/arXiv.2408.13687},
	abstract = {Quantum error correction provides a path to reach practical quantum computing by combining multiple physical qubits into a logical qubit, where the logical error rate is suppressed exponentially as more qubits are added. However, this exponential suppression only occurs if the physical error rate is below a critical threshold. In this work, we present two surface code memories operating below this threshold: a distance-7 code and a distance-5 code integrated with a real-time decoder. The logical error rate of our larger quantum memory is suppressed by a factor of \${\textbackslash}Lambda\$ = 2.14 \${\textbackslash}pm\$ 0.02 when increasing the code distance by two, culminating in a 101-qubit distance-7 code with 0.143\% \${\textbackslash}pm\$ 0.003\% error per cycle of error correction. This logical memory is also beyond break-even, exceeding its best physical qubit's lifetime by a factor of 2.4 \${\textbackslash}pm\$ 0.3. We maintain below-threshold performance when decoding in real time, achieving an average decoder latency of 63 \${\textbackslash}mu\$s at distance-5 up to a million cycles, with a cycle time of 1.1 \${\textbackslash}mu\$s. To probe the limits of our error-correction performance, we run repetition codes up to distance-29 and find that logical performance is limited by rare correlated error events occurring approximately once every hour, or 3 \${\textbackslash}times\$ 10\${\textasciicircum}9\$ cycles. Our results present device performance that, if scaled, could realize the operational requirements of large scale fault-tolerant quantum algorithms.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Acharya, Rajeev and Aghababaie-Beni, Laleh and Aleiner, Igor and Andersen, Trond I. and Ansmann, Markus and Arute, Frank and Arya, Kunal and Asfaw, Abraham and Astrakhantsev, Nikita and Atalaya, Juan and Babbush, Ryan and Bacon, Dave and Ballard, Brian and Bardin, Joseph C. and Bausch, Johannes and Bengtsson, Andreas and Bilmes, Alexander and Blackwell, Sam and Boixo, Sergio and Bortoli, Gina and Bourassa, Alexandre and Bovaird, Jenna and Brill, Leon and Broughton, Michael and Browne, David A. and Buchea, Brett and Buckley, Bob B. and Buell, David A. and Burger, Tim and Burkett, Brian and Bushnell, Nicholas and Cabrera, Anthony and Campero, Juan and Chang, Hung-Shen and Chen, Yu and Chen, Zijun and Chiaro, Ben and Chik, Desmond and Chou, Charina and Claes, Jahan and Cleland, Agnetta Y. and Cogan, Josh and Collins, Roberto and Conner, Paul and Courtney, William and Crook, Alexander L. and Curtin, Ben and Das, Sayan and Davies, Alex and Lorenzo, Laura De and Debroy, Dripto M. and Demura, Sean and Devoret, Michel and Paolo, Agustin Di and Donohoe, Paul and Drozdov, Ilya and Dunsworth, Andrew and Earle, Clint and Edlich, Thomas and Eickbusch, Alec and Elbag, Aviv Moshe and Elzouka, Mahmoud and Erickson, Catherine and Faoro, Lara and Farhi, Edward and Ferreira, Vinicius S. and Burgos, Leslie Flores and Forati, Ebrahim and Fowler, Austin G. and Foxen, Brooks and Ganjam, Suhas and Garcia, Gonzalo and Gasca, Robert and Genois, Élie and Giang, William and Gidney, Craig and Gilboa, Dar and Gosula, Raja and Dau, Alejandro Grajales and Graumann, Dietrich and Greene, Alex and Gross, Jonathan A. and Habegger, Steve and Hall, John and Hamilton, Michael C. and Hansen, Monica and Harrigan, Matthew P. and Harrington, Sean D. and Heras, Francisco J. H. and Heslin, Stephen and Heu, Paula and Higgott, Oscar and Hill, Gordon and Hilton, Jeremy and Holland, George and Hong, Sabrina and Huang, Hsin-Yuan and Huff, Ashley and Huggins, William J. and Ioffe, Lev B. and Isakov, Sergei V. and Iveland, Justin and Jeffrey, Evan and Jiang, Zhang and Jones, Cody and Jordan, Stephen and Joshi, Chaitali and Juhas, Pavol and Kafri, Dvir and Kang, Hui and Karamlou, Amir H. and Kechedzhi, Kostyantyn and Kelly, Julian and Khaire, Trupti and Khattar, Tanuj and Khezri, Mostafa and Kim, Seon and Klimov, Paul V. and Klots, Andrey R. and Kobrin, Bryce and Kohli, Pushmeet and Korotkov, Alexander N. and Kostritsa, Fedor and Kothari, Robin and Kozlovskii, Borislav and Kreikebaum, John Mark and Kurilovich, Vladislav D. and Lacroix, Nathan and Landhuis, David and Lange-Dei, Tiano and Langley, Brandon W. and Laptev, Pavel and Lau, Kim-Ming and Guevel, Loïck Le and Ledford, Justin and Lee, Kenny and Lensky, Yuri D. and Leon, Shannon and Lester, Brian J. and Li, Wing Yan and Li, Yin and Lill, Alexander T. and Liu, Wayne and Livingston, William P. and Locharla, Aditya and Lucero, Erik and Lundahl, Daniel and Lunt, Aaron and Madhuk, Sid and Malone, Fionn D. and Maloney, Ashley and Mandrá, Salvatore and Martin, Leigh S. and Martin, Steven and Martin, Orion and Maxfield, Cameron and McClean, Jarrod R. and McEwen, Matt and Meeks, Seneca and Megrant, Anthony and Mi, Xiao and Miao, Kevin C. and Mieszala, Amanda and Molavi, Reza and Molina, Sebastian and Montazeri, Shirin and Morvan, Alexis and Movassagh, Ramis and Mruczkiewicz, Wojciech and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Nersisyan, Ani and Neven, Hartmut and Newman, Michael and Ng, Jiun How and Nguyen, Anthony and Nguyen, Murray and Ni, Chia-Hung and O'Brien, Thomas E. and Oliver, William D. and Opremcak, Alex and Ottosson, Kristoffer and Petukhov, Andre and Pizzuto, Alex and Platt, John and Potter, Rebecca and Pritchard, Orion and Pryadko, Leonid P. and Quintana, Chris and Ramachandran, Ganesh and Reagor, Matthew J. and Rhodes, David M. and Roberts, Gabrielle and Rosenberg, Eliott and Rosenfeld, Emma and Roushan, Pedram and Rubin, Nicholas C. and Saei, Negar and Sank, Daniel and Sankaragomathi, Kannan and Satzinger, Kevin J. and Schurkus, Henry F. and Schuster, Christopher and Senior, Andrew W. and Shearn, Michael J. and Shorter, Aaron and Shutty, Noah and Shvarts, Vladimir and Singh, Shraddha and Sivak, Volodymyr and Skruzny, Jindra and Small, Spencer and Smelyanskiy, Vadim and Smith, W. Clarke and Somma, Rolando D. and Springer, Sofia and Sterling, George and Strain, Doug and Suchard, Jordan and Szasz, Aaron and Sztein, Alex and Thor, Douglas and Torres, Alfredo and Torunbalci, M. Mert and Vaishnav, Abeer and Vargas, Justin and Vdovichev, Sergey and Vidal, Guifre and Villalonga, Benjamin and Heidweiller, Catherine Vollgraff and Waltman, Steven and Wang, Shannon X. and Ware, Brayden and Weber, Kate and White, Theodore and Wong, Kristi and Woo, Bryan W. K. and Xing, Cheng and Yao, Z. Jamie and Yeh, Ping and Ying, Bicheng and Yoo, Juhwan and Yosri, Noureldin and Young, Grayson and Zalcman, Adam and Zhang, Yaxing and Zhu, Ningfeng and Zobrist, Nicholas},
	month = aug,
	year = {2024},
	note = {arXiv:2408.13687 [quant-ph]},
	keywords = {Quantum Physics},
	annote = {Comment: 10 pages, 4 figures, Supplementary Information},
	file = {Preprint PDF:C\:\\Users\\alex\\Zotero\\storage\\SQ92UXPJ\\Acharya и др. - 2024 - Quantum error correction below the surface code threshold.pdf:application/pdf;Snapshot:C\:\\Users\\alex\\Zotero\\storage\\9YGIZ9SS\\2408.html:text/html},
}

@misc{QuantumErrorCorrection,
	title = {Quantum {Error} {Correction} {\textbar} {QuEra}},
	url = {https://www.quera.com/qec},
	abstract = {Using breakthrough technology, we are pleased to unveil a line of quantum computers with quantum error correction.},
	language = {en},
	urldate = {2024-12-04},
	file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\5ZYT72VN\\qec.html:text/html},
}

@article{zhangGenerativeQuantumMachine2024,
	title = {Generative {Quantum} {Machine} {Learning} via {Denoising} {Diffusion} {Probabilistic} {Models}},
	volume = {132},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.132.100602},
	doi = {10.1103/PhysRevLett.132.100602},
	abstract = {Deep generative models are key-enabling technology to computer vision, text generation, and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and a relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the quantum denoising diffusion probabilistic model (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while it introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and demonstrate QuDDPM’s capability in learning correlated quantum noise model, quantum many-body phases, and topological structure of quantum data. The results provide a paradigm for versatile and efficient quantum generative learning.},
	number = {10},
	urldate = {2024-12-04},
	journal = {Physical Review Letters},
	author = {Zhang, Bingzhi and Xu, Peng and Chen, Xiaohui and Zhuang, Quntao},
	month = mar,
	year = {2024},
	note = {Publisher: American Physical Society},
	pages = {100602},
	file = {Принятая версия:C\:\\Users\\alex\\Zotero\\storage\\H8U7A22J\\Zhang и др. - 2024 - Generative Quantum Machine Learning via Denoising Diffusion Probabilistic Models.pdf:application/pdf;APS Snapshot:C\:\\Users\\alex\\Zotero\\storage\\A8E7SM4C\\PhysRevLett.132.html:text/html},
}

@misc{kolleQuantumDenoisingDiffusion2024,
	title = {Quantum {Denoising} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2401.07049},
	doi = {10.48550/arXiv.2401.07049},
	abstract = {In recent years, machine learning models like DALL-E, Craiyon, and Stable Diffusion have gained significant attention for their ability to generate high-resolution images from concise descriptions. Concurrently, quantum computing is showing promising advances, especially with quantum machine learning which capitalizes on quantum mechanics to meet the increasing computational requirements of traditional machine learning algorithms. This paper explores the integration of quantum machine learning and variational quantum circuits to augment the efficacy of diffusion-based image generation models. Specifically, we address two challenges of classical diffusion models: their low sampling speed and the extensive parameter requirements. We introduce two quantum diffusion models and benchmark their capabilities against their classical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our models surpass the classical models with similar parameter counts in terms of performance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency model unitary single sampling architecture that combines the diffusion procedure into a single step, enabling a fast one-step image generation.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Kölle, Michael and Stenzel, Gerhard and Stein, Jonas and Zielinski, Sebastian and Ommer, Björn and Linnhoff-Popien, Claudia},
	month = jan,
	year = {2024},
	note = {arXiv:2401.07049 [quant-ph]},
	keywords = {Quantum Physics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\alex\\Zotero\\storage\\G7AX4H8M\\Kölle и др. - 2024 - Quantum Denoising Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\alex\\Zotero\\storage\\C284MIIG\\2401.html:text/html},
}

@article{cherratQuantumVisionTransformers2024,
	title = {Quantum {Vision} {Transformers}},
	volume = {8},
	url = {https://quantum-journal.org/papers/q-2024-02-22-1265/},
	doi = {10.22331/q-2024-02-22-1265},
	abstract = {El Amine Cherrat, Iordanis Kerenidis, Natansh Mathur, Jonas Landman, Martin Strahm, and Yun Yvonna Li,
Quantum 8, 1265 (2024).
In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in nat…},
	language = {en-GB},
	urldate = {2024-12-04},
	journal = {Quantum},
	author = {Cherrat, El Amine and Kerenidis, Iordanis and Mathur, Natansh and Landman, Jonas and Strahm, Martin and Li, Yun Yvonna},
	month = feb,
	year = {2024},
	note = {Publisher: Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
	pages = {1265},
	file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\TKZEDIUN\\Cherrat и др. - 2024 - Quantum Vision Transformers.pdf:application/pdf},
}

@misc{khatriQuixerQuantumTransformer2024,
	title = {Quixer: {A} {Quantum} {Transformer} {Model}},
	shorttitle = {Quixer},
	url = {http://arxiv.org/abs/2406.04305},
	doi = {10.48550/arXiv.2406.04305},
	abstract = {Progress in the realisation of reliable large-scale quantum computers has motivated research into the design of quantum machine learning models. We present Quixer: a novel quantum transformer model which utilises the Linear Combination of Unitaries and Quantum Singular Value Transform primitives as building blocks. Quixer operates by preparing a superposition of tokens and applying a trainable non-linear transformation to this mix. We present the first results for a quantum transformer model applied to a practical language modelling task, obtaining results competitive with an equivalent classical baseline. In addition, we include resource estimates for evaluating the model on quantum hardware, and provide an open-source implementation for classical simulation. We conclude by highlighting the generality of Quixer, showing that its parameterised components can be substituted with fixed structures to yield new classes of quantum transformers.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Khatri, Nikhil and Matos, Gabriel and Coopmans, Luuk and Clark, Stephen},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04305 [quant-ph]},
	keywords = {Quantum Physics},
	annote = {Comment: 17 pages, 8 figures},
	file = {Preprint PDF:C\:\\Users\\alex\\Zotero\\storage\\TPD2VLRU\\Khatri и др. - 2024 - Quixer A Quantum Transformer Model.pdf:application/pdf;Snapshot:C\:\\Users\\alex\\Zotero\\storage\\Z7RAZ4JT\\2406.html:text/html},
}

@article{zhangTransformerQuantumState2023,
	title = {Transformer quantum state: {A} multipurpose model for quantum many-body problems},
	volume = {107},
	shorttitle = {Transformer quantum state},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.107.075147},
	doi = {10.1103/PhysRevB.107.075147},
	abstract = {Inspired by the advancements in large language models based on transformers, we introduce the transformer quantum state (TQS): a versatile machine learning model for quantum many-body problems. In sharp contrast to Hamiltonian/task specific models, TQS can generate the entire phase diagram, predict field strengths with experimental measurements, and transfer such a knowledge to new systems it has never been trained on before, all within a single model. With specific tasks, fine-tuning the TQS produces accurate results with small computational cost. Versatile by design, TQS can be easily adapted to new tasks, thereby pointing towards a general-purpose model for various challenging quantum problems.},
	number = {7},
	urldate = {2024-12-04},
	journal = {Physical Review B},
	author = {Zhang, Yuan-Hang and Di Ventra, Massimiliano},
	month = feb,
	year = {2023},
	note = {Publisher: American Physical Society},
	pages = {075147},
	file = {Отправленная версия:C\:\\Users\\alex\\Zotero\\storage\\4C6T77AX\\Zhang и Di Ventra - 2023 - Transformer quantum state A multipurpose model for quantum many-body problems.pdf:application/pdf;APS Snapshot:C\:\\Users\\alex\\Zotero\\storage\\A5NV2YHR\\PhysRevB.107.html:text/html},
}

@article{trugenbergerProbabilisticQuantumMemories2001,
	title = {Probabilistic {Quantum} {Memories}},
	volume = {87},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.87.067901},
	doi = {10.1103/PhysRevLett.87.067901},
	abstract = {Typical address-oriented computer memories cannot recognize incomplete or noisy information. Associative (content-addressable) memories solve this problem but suffer from severe capacity shortages. I propose a model of a quantum memory that solves both problems. The storage capacity is exponential in the number of qbits and thus optimal. The retrieval mechanism for incomplete or noisy inputs is probabilistic, with postselection of the measurement result. The output is determined by a probability distribution on the memory which is peaked around the stored patterns closest in Hamming distance to the input.},
	number = {6},
	urldate = {2024-12-04},
	journal = {Physical Review Letters},
	author = {Trugenberger, C. A.},
	month = jul,
	year = {2001},
	note = {Publisher: American Physical Society},
	pages = {067901},
	file = {Отправленная версия:C\:\\Users\\alex\\Zotero\\storage\\TKEW6M7B\\Trugenberger - 2001 - Probabilistic Quantum Memories.pdf:application/pdf;APS Snapshot:C\:\\Users\\alex\\Zotero\\storage\\CBRJU2RY\\PhysRevLett.87.html:text/html},
}

@misc{GPT4,
	title = {{GPT}-4},
	url = {https://openai.com/index/gpt-4/},
	abstract = {It can generate, edit, and iterate with users on creative and technical writing tasks, such as composing songs, writing screenplays, or learning a user’s writing style.},
	language = {en-US},
	urldate = {2024-12-05},
	file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\FTJ2E82Q\\gpt-4.html:text/html},
}

@misc{NobelPrizePhysicsa,
	title = {The {Nobel} {Prize} in {Physics} 2022},
	url = {https://www.nobelprize.org/prizes/physics/2022/summary/},
	abstract = {The Nobel Prize in Physics 2022 was awarded jointly to Alain Aspect, John F. Clauser and Anton Zeilinger "for experiments with entangled photons, establishing the violation of Bell inequalities and  pioneering quantum information science"},
	language = {en-US},
	urldate = {2024-12-05},
	journal = {NobelPrize.org},
	file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\AY6FIVLZ\\summary.html:text/html},
}

@misc{BreakthroughPrizeWinners,
	title = {Breakthrough {Prize} – {Winners} {Of} {The} 2023 {Breakthrough} {Prizes} {In} {Life} {Sciences}, {Mathematics} {And} {Fundamental} {Physics} {Announced}},
	url = {https://breakthroughprize.org/News/73},
	language = {en},
	urldate = {2024-12-05},
	file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\PEC8FBJE\\73.html:text/html},
}

@article{nguyenBayesianQuantumNeural2022,
	title = {Bayesian {Quantum} {Neural} {Networks}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9759399},
	doi = {10.1109/ACCESS.2022.3168675},
	abstract = {The astounding acceleration in Artificial Intelligence and Quantum Computing advances naturally gives rise to a line of research, which unrolls the potential advantages of quantum computing on classical Machine Learning tasks, known as Quantum Machine Learning or Quantum Machine Intelligence. The typical objectives are either (1) exploring the potential quantum advantages on classical learning tasks or (2) levering well-established classical ML algorithms to tackle quantum-related problems on Noisy Intermediate-Scale Quantum (NISQ) devices. Along the second research direction, we study Quantum Neural Networks (QNNs) to accomplish the purpose of Bayesian learning. By observing a wide range of studies on QNNs, in which the sole training method is based on frequentist training, we find that Bayesian learning benefits QNNs from two aspects. First, Bayesian-trained models enjoy a high level of generalization due to the prior and posterior distribution usage compared to frequentist training, which will be justified by this paper’s theoretical study of model capacity. Second, Bayesian Inference offers epistemic uncertainty estimation, which merits the decision-making process. It is worth mentioning that frequentist-trained QNNs generally lack this desirable property. Under the Bayesian training procedure, our derived models can be considered a new class of QNNs (called BayesianQNNs) which possesses both desirable properties of Bayesian Inference while maintaining comparable predictive performance as frequentist counterparts. The proposed Bayesian Quantum Neural Networks is justified by empirical evidence from numerical experiments.},
	urldate = {2024-12-05},
	journal = {IEEE Access},
	author = {Nguyen, Nam and Chen, Kwang-Cheng},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Bayes methods, Bayesian inference, Bayesian learning, Data models, Integrated circuit modeling, Machine learning, Neural networks, Quantum computing, Quantum machine learning, quantum neural networks, Training},
	pages = {54110--54122},
	file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\JE9ECJU2\\Nguyen и Chen - 2022 - Bayesian Quantum Neural Networks.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\alex\\Zotero\\storage\\58MXZ99B\\9759399.html:text/html},
}

@article{daiQuantumBayesianOptimization2023,
	title = {Quantum {Bayesian} {Optimization}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/401aa72e0e3be680348a5b0ffdb1a5aa-Abstract-Conference.html},
	language = {en},
	urldate = {2024-12-05},
	journal = {Advances in Neural Information Processing Systems},
	author = {Dai, Zhongxiang and Lau, Gregory Kang Ruey and Verma, Arun and Shu, Yao and Low, Bryan Kian Hsiang and Jaillet, Patrick},
	month = dec,
	year = {2023},
	pages = {20179--20207},
	file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\E7CGXQA9\\Dai и др. - 2023 - Quantum Bayesian Optimization.pdf:application/pdf},
}

@misc{QuantinuumsHSeriesHits,
	title = {Quantinuum’s {H}-{Series} hits 56 physical qubits that are all-to-all connected, and departs the era of classical simulation},
	url = {https://www.quantinuum.com/blog/quantinuums-h-series-hits-56-physical-qubits-that-are-all-to-all-connected-and-departs-the-era-of-classical-simulation},
	abstract = {Quantinuum’s H-Series combines full scalability with market-leading fidelity, performance, and error correction capabilities. In a demonstration with JPMorgan Chase \& Co., Quantinuum’s H2-1 with 56 qubits achieved a massive uplift in an iconic demonstration.},
	language = {en},
	urldate = {2024-12-08},
	file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\3QEHQWUD\\quantinuums-h-series-hits-56-physical-qubits-that-are-all-to-all-connected-and-departs-the-era-.html:text/html},
}

@inproceedings{zhuangFastTrainingTripletBased2016,
	title = {Fast {Training} of {Triplet}-{Based} {Deep} {Binary} {Embedding} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Zhuang_Fast_Training_of_CVPR_2016_paper.html},
	urldate = {2024-12-08},
	author = {Zhuang, Bohan and Lin, Guosheng and Shen, Chunhua and Reid, Ian},
	year = {2016},
	pages = {5955--5964},
	file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\HKJN6EHD\\Zhuang и др. - 2016 - Fast Training of Triplet-Based Deep Binary Embedding Networks.pdf:application/pdf},
}

@inproceedings{yiBinaryEmbeddingFundamental2015,
	title = {Binary {Embedding}: {Fundamental} {Limits} and {Fast} {Algorithm}},
	shorttitle = {Binary {Embedding}},
	url = {https://proceedings.mlr.press/v37/yi15.html},
	abstract = {Binary embedding is a nonlinear dimension reduction methodology where high dimensional data are embedded into the Hamming cube while preserving the structure of the original space. Specifically, for an arbitrary N distinct points in {\textbackslash}mathbbS{\textasciicircum}p-1, our goal is to encode each point using m-dimensional binary strings such that we can reconstruct their geodesic distance up to δuniform distortion. Existing binary embedding algorithms either lack theoretical guarantees or suffer from running time O(mp). We make three contributions: (1) we establish a lower bound that shows any binary embedding oblivious to the set of points requires m =Ω({\textbackslash}frac1δ{\textasciicircum}2{\textbackslash}logN) bits and a similar lower bound for non-oblivious embeddings into Hamming distance; (2) we propose a novel fast binary embedding algorithm with provably optimal bit complexity m = O({\textbackslash}frac1 δ{\textasciicircum}2{\textbackslash}logN) and near linear running time O(p {\textbackslash}log p) whenever {\textbackslash}log N ≪δ{\textbackslash}sqrtp, with a slightly worse running time for larger {\textbackslash}log N; (3) we also provide an analytic result about embedding a general set of points K ⊆{\textbackslash}mathbbS{\textasciicircum}p-1 with even infinite size. Our theoretical findings are supported through experiments on both synthetic and real data sets.},
	language = {en},
	urldate = {2024-12-08},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yi, Xinyang and Caramanis, Constantine and Price, Eric},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {2162--2170},
	file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\HZ5JFUPC\\Yi и др. - 2015 - Binary Embedding Fundamental Limits and Fast Algorithm.pdf:application/pdf},
}

@misc{AsymmetricDistancesBinary,
	title = {Asymmetric {Distances} for {Binary} {Embeddings} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/6518116},
	urldate = {2024-12-08},
	file = {Asymmetric Distances for Binary Embeddings | IEEE Journals & Magazine | IEEE Xplore:C\:\\Users\\alex\\Zotero\\storage\\RB44G6N7\\6518116.html:text/html},
}
