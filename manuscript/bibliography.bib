% Encoding: UTF-8
@article{akshay2020,
  title = {Reachability Deficits in Quantum Approximate Optimization},
  author = {V. Akshay and H. Philathong and M.{\hspace{0.167em}}E.{\hspace{0.167em}}S. Morales and J.{\hspace{0.167em}}D. Biamonte},
  year = {2020},
  month = {mar},
  journal = {Physical Review Letters},
  publisher = {American Physical Society ({APS})},
  volume = {124},
  number = {9},
  pages = {090504},
  optdoi = {10.1103/physrevlett.124.090504},
}

@article{Liu2022,
  title = {Quantum capsule networks},
  author = {Zidu Liu and Pei-Xin Shen and Weikang Li and L-M Duan and Dong-Ling Deng},
  year = {2022},
  month = {dec},
  journal = {Quantum Science and Technology},
  publisher = {{IOP} Publishing},
  volume = {8},
  number = {1},
  pages = {015016},
  doi = {10.1088/2058-9565/aca55d},
}

@article{Heese2022,
  title = {Representation of binary classification trees with binary features by quantum circuits},
  author = {Raoul Heese and Patricia Bickert and Astrid Elisa Niederle},
  year = {2022},
  month = {mar},
  journal = {Quantum},
  publisher = {Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
  volume = {6},
  pages = {676},
  doi = {10.22331/q-2022-03-30-676},
}

@article{Viteritti2023,
  title = {Transformer Variational Wave Functions for Frustrated Quantum Spin Systems},
  author = {Luciano Loris Viteritti and Riccardo Rende and Federico Becca},
  year = {2023},
  month = {jun},
  journal = {Physical Review Letters},
  publisher = {American Physical Society ({APS})},
  volume = {130},
  number = {23},
  pages = {236401},
  doi = {10.1103/physrevlett.130.236401},
}

@article{Benedetti2019,
  title = {Parameterized quantum circuits as machine learning models},
  author = {Marcello Benedetti and Erika Lloyd and Stefan Sack and Mattia Fiorentini},
  year = {2019},
  month = {nov},
  journal = {Quantum Science and Technology},
  publisher = {{IOP} Publishing},
  volume = {4},
  number = {4},
  pages = {043001},
  doi = {10.1088/2058-9565/ab4eb5},
}

@article{Sajjan2022,
  title = {Quantum machine learning for chemistry and physics},
  author = {Manas Sajjan and Junxu Li and Raja Selvarajan and Shree Hari Sureshbabu and Sumit Suresh Kale and Rishabh Gupta and Vinit Singh and Sabre Kais},
  year = {2022},
  journal = {Chemical Society Reviews},
  publisher = {Royal Society of Chemistry ({RSC})},
  volume = {51},
  number = {15},
  pages = {6475--6573},
  doi = {10.1039/d2cs00203e},
}

@article{Cerezo2022,
  title = {Challenges and opportunities in quantum machine learning},
  author = {M. Cerezo and Guillaume Verdon and Hsin-Yuan Huang and Lukasz Cincio and Patrick J. Coles},
  year = {2022},
  month = {sep},
  journal = {Nature Computational Science},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {2},
  number = {9},
  pages = {567--576},
  doi = {10.1038/s43588-022-00311-3},
}

@article{Bharti2022,
  title = {Noisy intermediate-scale quantum algorithms},
  author = {Kishor Bharti and Alba Cervera-Lierta and Thi Ha Kyaw and Tobias Haug and Sumner Alperin-Lea and Abhinav Anand and Matthias Degroote and Hermanni Heimonen and Jakob S. Kottmann and Tim Menke and Wai-Keong Mok and Sukin Sim and Leong-Chuan Kwek and Al{\'{a}}n Aspuru-Guzik},
  year = {2022},
  month = {feb},
  journal = {Reviews of Modern Physics},
  publisher = {American Physical Society ({APS})},
  volume = {94},
  number = {1},
  pages = {015004},
  doi = {10.1103/revmodphys.94.015004},
}

@article{Cerezo2021,
  title = {Variational quantum algorithms},
  author = {M. Cerezo and Andrew Arrasmith and Ryan Babbush and Simon C. Benjamin and Suguru Endo and Keisuke Fujii and Jarrod R. McClean and Kosuke Mitarai and Xiao Yuan and Lukasz Cincio and Patrick J. Coles},
  year = {2021},
  month = {aug},
  journal = {Nature Reviews Physics},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {3},
  number = {9},
  pages = {625--644},
  doi = {10.1038/s42254-021-00348-9},
}

@misc{sevilla2022compute,
  title = {Compute Trends Across Three Eras of Machine Learning},
  author = {Jaime Sevilla and Lennart Heim and Anson Ho and Tamay Besiroglu and Marius Hobbhahn and Pablo Villalobos},
  year = {2022},
  eprint = {2202.05924},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
}

@article{allcock2018,
  title = {Quantum algorithms for feedforward neural networks},
  author = {{Allcock}, Jonathan and {Hsieh}, Chang-Yu and {Kerenidis}, Iordanis and {Zhang}, Shengyu},
  year = {2018},
  journal = {arXiv:1812.03089},
}

% no volume
@article{appiah2009,
  title = {A binary Self-Organizing Map and its {FPGA} implementation},
  author = {Kofi Appiah and Andrew Hunter and  Hongying Meng and  Shigang Yue and Mervyn Hobden and Nigel Priestley and Peter Hobden and Cy Pettit},
  year = {2009},
  month = {jun},
  journal = {2009 International Joint Conference on Neural Networks},
  publisher = {{IEEE}},
  pages = {164--171},
  optdoi = {10.1109/ijcnn.2009.5179001},
  opturl = {https://doi.org/10.1109\%2Fijcnn.2009.5179001},
}

@article{arute2020,
  title = {Quantum Approximate Optimization of Non-Planar Graph Problems on a Planar Superconducting Processor},
  author = {Frank Arute and Kunal Arya and Ryan Babbush and Dave Bacon and Joseph C. Bardin and Rami Barends and Sergio Boixo and Michael Broughton and Bob B. Buckley and David A. Buell and Brian Burkett and Nicholas Bushnell and Yu Chen and Zijun Chen and Ben Chiaro and Roberto Collins and William Courtney and Sean Demura and Andrew Dunsworth and Edward Farhi and Austin Fowler and Brooks Foxen and Craig Gidney and Marissa Giustina and Rob Graff and Steve Habegger and Matthew P. Harrigan and Alan Ho and Sabrina Hong and Trent Huang and L. B. Ioffe and Sergei V. Isakov and Evan Jeffrey and Zhang Jiang and Cody Jones and Dvir Kafri and Kostyantyn Kechedzhi and Julian Kelly and Seon Kim and Paul V. Klimov and Alexander N. Korotkov and Fedor Kostritsa and David Landhuis and Pavel Laptev and Mike Lindmark and Martin Leib and Erik Lucero and Orion Martin and John M. Martinis and Jarrod R. McClean and Matt McEwen and Anthony Megrant and Xiao Mi and Masoud Mohseni and Wojciech Mruczkiewicz and Josh Mutus and Ofer Naaman and Matthew Neeley and Charles Neill and Florian Neukart and Hartmut Neven and Murphy Yuezhen Niu and Thomas E. O'Brien and Bryan O'Gorman and Eric Ostby and Andre Petukhov and Harald Putterman and Chris Quintana and Pedram Roushan and Nicholas C. Rubin and Daniel Sank and Kevin J. Satzinger and Andrea Skolik and Vadim Smelyanskiy and Doug Strain and Michael Streif and Kevin J. Sung and Marco Szalay and Amit Vainsencher and Theodore White and Z. Jamie Yao and Ping Yeh and Adam Zalcman and Leo Zhou},
  year = {2020},
  journal = {arXiv: 2004.04197},
}

@article{aspuru-guzik2005,
  title = {Simulated Quantum Computation of Molecular Energies},
  author = {A. Aspuru-Guzik},
  year = {2005},
  month = {sep},
  journal = {Science},
  publisher = {American Association for the Advancement of Science ({AAAS})},
  volume = {309},
  number = {5741},
  pages = {1704--1707},
  optdoi = {10.1126/science.1113479},
}

@article{biamonte2017,
  title = {Quantum machine learning},
  author = {Jacob Biamonte and Peter Wittek and Nicola Pancotti and Patrick Rebentrost and Nathan Wiebe and Seth Lloyd},
  year = {2017},
  month = {sep},
  journal = {Nature},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {549},
  number = {7671},
  pages = {195--202},
  optdoi = {10.1038/nature23474},
}

@article{bio0,
  title = {Circular RNA TTN Acts As a miR-432 Sponge to Facilitate Proliferation and Differentiation of Myoblasts via the IGF2/PI3K/AKT Signaling Pathway},
  author = {Xiaogang Wang and Xiukai Cao and Dong Dong and Xuemei Shen and Jie Cheng and Rui Jiang and Zhaoxin Yang and Shujun Peng and Yongzhen Huang and Xianyong Lan and Ibrahim Elsaeid Elnour and Chuzhao Lei and Hong Chen},
  year = {2019},
  journal = {Molecular Therapy - Nucleic Acids},
  volume = {18},
  pages = {966--980},
  issn = {2162-2531},
  abstract = {Circular RNAs (circRNAs) are ubiquitous endogenous RNA found in various organisms that can regulate gene expression in eukaryotes. However, little is known about potential roles for circRNAs in muscle development. We analyzed circRNA sequencing data of bovine skeletal muscle tissue and found differential expression of circTitin (circTTN) in fetal and adult bovine muscle tissue. We then further studied the role of circTTN in bovine muscle development. Overexpression and inhibition of circTTN together elicited its promoting roles in proliferation and differentiation of bovine primary myoblasts. Mechanistically, circTTN showed interaction with miR-432 by luciferase screening and RNA immunoprecipitation (RIP) assays. Additionally, miR-432 is a regulator of insulin-like growth factor 2 (IGF2), as indicated by luciferase activity, quantitative real-time PCR, and western blotting assays. Increased miR-432 expression inhibited the expression of IGF2, but this effect was remitted by circTTN. Conclusively, our results showed that circTTN promoted proliferation and differentiation of bovine primary myoblasts via competitively combining with miR-432 to activate the IGF2/phosphatidylinositol 3-kinase (PI3K)/AKT signaling pathway.},
  optdoi = {https://doi.org/10.1016/j.omtn.2019.10.019},
  keywords = {bovine, circRNA, miRNA,  pathway, myoblast},
  opturl = {http://www.sciencedirect.com/science/article/pii/S2162253119303294},
}

@article{bio1,
  title = {Eukaryotic elongation factor 2 is involved in the anticoccidial action of diclazuril in the second-generation merozoites of Eimeria tenella},
  author = {Bian Zhou and Liu-shu Jia and Hong-wei Guo and Hai-yan Ding and Jing-yun Yang and Hong-wei Wang},
  year = {2019},
  month = {dec},
  journal = {Veterinary Parasitology},
  publisher = {Elsevier {BV}},
  volume = {276},
  pages = {108991},
  abstract = {Eimeria tenella, an obligate intracellular parasite, can actively invade the cecal epithelial cells of chickens and cause severe enteric disease. Eukaryotic elongation factor 2 (eEF2) plays a major role in protein synthesis and cell survival. This study aims to explore the exact mechanisms underlying diclazuril inhibition in second-generation merozoites of E. tenella. The eEF2 cDNA of the second-generation merozoites of E. tenella (EtEF2) was cloned by reverse transcriptase polymerase chain reaction and rapid amplification of cDNA ends. Diclazuril-induced expression profiles of EtEF2 were also analyzed. The cloned full-length cDNA (2893 bp) of the EtEF2 nucleotide sequence encompassed a 2499 bp open reading frame (ORF) that encoded a polypeptide of 832 residues with an estimated molecular mass of 93.12\hspace{0.167em}kDa and a theoretical isoelectric point of 5.99. The EtEF2 nucleotide sequence was submitted to the GenBank database with the accession number KF188423. The EtEF2 protein sequence shared 99 \% homology with the eEF2 sequence of Toxoplasma gondii (GenBank XP\_002367778.1). The GTPase activity domain and ADP-ribosylation domain were conserved signature sequences of the eEF2 gene family. The changes in the transcriptional and translational levels of EtEF2 were detected through quantitative real-time PCR and Western blot analyses. The mRNA expression level of EtEF2 was 2.706 fold increases and the protein level of EtEF2 was increased 67.31 \% under diclazuril treatment. In addition, the localization of EtEF2 was investigated through immunofluorescence assay. Experimental results demonstrated that EtEF2 was distributed primarily in the cytoplasm of second-generation merozoites, and its fluorescence intensity was enhanced after diclazuril treatment. These findings indicated that EtEF2 may have an important role in understanding the signaling mechanism underlying the anticoccidial action of diclazuril and could be a promising target for novel drug exploration.},
  optdoi = {10.1016/j.vetpar.2019.108991},
  keywords = {, Diclazuril, Eukaryotic elongation factor 2, Second-generation merozoites},
  opturl = {https://doi.org/10.1016\%2Fj.vetpar.2019.108991},
}

@article{bio2,
  title = {Correlation of {CpG} methylation of the Pdcd1 gene with {PD}-1 expression on {CD}8 + T cells and medical laboratory indicators in chronic hepatitis B infection},
  author = {Lin Jiao and Jie Chen and Xiaojuan Wu and Bei Cai and Zhenzhen Su and Lanlan Wang},
  year = {2020},
  month = {jan},
  journal = {The Journal of Gene Medicine},
  publisher = {Wiley},
  volume = {22},
  number = {2},
  pages = {e3148},
  abstract = {Abstract Background The negative signal provided by some co-inhibitory factors such as programmed cell death-1 (PD-1) has been associated with chronic hepatitis B (CHB) infection induced-T cell exhaustion, although the correlation of CpG methylation of the Pdcd1 gene with PD-1 expression and medical laboratory indicators in CHB infection has not yet been elucidated. Methods Blood samples from 20 CHB infection patients and 20 spontaneous clearance (SC) patients were collected. Percentages of PD-1-positive CD8+ T cells were analyzed by flow cytometry. The percentage of CpG methylation at the Pdcd1 locus was analyzed by bisulfite sequencing. Student's t test, Pearson and Spearman's correlation, and Mann–Whitney tests were used in the statistical analysis. Results Percentages of PD-1-positive CD8+ T cells in peripheral blood T cells were significantly higher in CHB patients than in the SC group (p < 0.001). The methylation level of Pdcd1 was significantly lower in CHB patients (p < 0.001) and the methylation level of Pdcd1 was negatively correlated with PD-1 expression level in CD8+ T cells (p < 0.001) and hepatitis-B surface antigen (HBsAg) (p < 0.001). Conclusions The results of the present study suggest that Pdcd1 methylation is correlated with PD-1 expression on CD8+ T cells and correlated with HBsAg and alanine aminotransferase. The results may provide new ideas regarding anti-PD-1 inhibitors, and epigenetic regulators such as demethylation inhibitors could represent more successful therapeutic strategies in hepatitis B infection patients.},
  optdoi = {10.1002/jgm.3148},
  opturl = {https://doi.org/10.1002\%2Fjgm.3148},
}

@article{bondarenko2019,
  title = {Quantum autoencoders to denoise quantum data},
  author = {Dmytro Bondarenko and Polina Feldmann},
  year = {2019},
  journal = {arXiv: 1910.09169},
}

@article{broughton2020,
  title = {TensorFlow Quantum: A Software Framework for Quantum Machine Learning},
  author = {Michael Broughton and Guillaume Verdon and Trevor McCourt and Antonio J. Martinez and Jae Hyeon Yoo and Sergei V. Isakov and Philip Massey and Murphy Yuezhen Niu and Ramin Halavati and Evan Peters and Martin Leib and Andrea Skolik and Michael Streif and David Von Dollen and Jarrod R. McClean and Sergio Boixo and Dave Bacon and Alan K. Ho and Hartmut Neven and Masoud Mohseni},
  year = {2020},
  journal = {arXiv: 2003.02989},
}

@article{butler2018,
  title = {Machine learning for molecular and materials science},
  author = {Butler, Keith T. and Davies, Daniel W. and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron},
  year = {2018},
  journal = {Nature},
  volume = {559},
  number = {7715},
  pages = {547--555},
  issn = {1476-4687},
  abstract = {Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence.},
  optdoi = {10.1038/s41586-018-0337-2},
  opturl = {https://doi.org/10.1038/s41586-018-0337-2},
}

@article{byrnes2013,
  title = {Neural networks using two-component Bose-Einstein condensates},
  author = {Tim Byrnes and Shinsuke Koyama and Kai Yan and Yoshihisa Yamamoto},
  year = {2013},
  month = {aug},
  journal = {Scientific Reports},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {3},
  number = {1},
  pages = {2531},
  optdoi = {10.1038/srep02531},
}

@article{carleo2019,
  title = {Machine learning and the physical sciences},
  author = {Giuseppe Carleo and Ignacio Cirac and Kyle Cranmer and Laurent Daudet and Maria Schuld and Naftali Tishby and Leslie Vogt-Maranto and Lenka Zdeborov{\'{a}}},
  year = {2019},
  month = {dec},
  journal = {Reviews of Modern Physics},
  publisher = {American Physical Society ({APS})},
  volume = {91},
  number = {4},
  pages = {045002},
  optdoi = {10.1103/revmodphys.91.045002},
}

@article{chea2016,
  title = {Evidence of Water Quality Degradation in Lower Mekong Basin Revealed by Self-Organizing Map},
  author = {Ratha Chea and Ga\"{e}l Grenouillet and Sovan Lek},
  year = {2016},
  month = {jan},
  journal = {{PLOS} {ONE}},
  publisher = {Public Library of Science ({PLoS})},
  volume = {11},
  number = {1},
  pages = {e0145527},
  optdoi = {10.1371/journal.pone.0145527},
  editor = {Chon-Lin Lee},
}

@article{cherny2019,
  title = {Nontrivial Attractors of the Perturbed Nonlinear Schr\"{o}dinger Equation: Applications to Associative Memory and Pattern Recognition},
  author = {Valentin V. Cherny and Tim Byrnes and Alexey N. Pyrkov},
  year = {2019},
  month = {feb},
  journal = {Advanced Quantum Technologies},
  publisher = {Wiley},
  volume = {2},
  number = {7-8},
  pages = {1800087},
  optdoi = {10.1002/qute.201800087},
}

@article{childs2017,
  title = {Quantum Algorithm for Systems of Linear Equations with Exponentially Improved Dependence on Precision},
  author = {Andrew M. Childs and Robin Kothari and Rolando D. Somma},
  year = {2017},
  month = {jan},
  journal = {{SIAM} Journal on Computing},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  volume = {46},
  number = {6},
  pages = {1920--1950},
  optdoi = {10.1137/16m1087072},
}

@article{cong2019,
  title = {Quantum convolutional neural networks},
  author = {Iris Cong and Soonwon Choi and Mikhail D. Lukin},
  year = {2019},
  month = {aug},
  journal = {Nature Physics},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {15},
  number = {12},
  pages = {1273--1278},
  optdoi = {10.1038/s41567-019-0648-8},
}

@article{corsello2017,
  title = {The Drug Repurposing Hub: a next-generation drug library and information resource},
  author = {Steven M Corsello and Joshua A Bittker and Zihan Liu and Joshua Gould and Patrick McCarren and Jodi E Hirschman and Stephen E Johnston and Anita Vrcic and Bang Wong and Mariya Khan and Jacob Asiedu and Rajiv Narayan and Christopher C Mader and Aravind Subramanian and Todd R Golub},
  year = {2017},
  month = {apr},
  journal = {Nature Medicine},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {23},
  number = {4},
  pages = {405--408},
  optdoi = {10.1038/nm.4306},
}

@article{doszkocs1990,
  title = {Connectionist models and information retrieval. Annual Review of Information Science and Technology},
  author = {Doszkocs, T. E and  Reggia, J and Lin, X},
  year = {1990},
  journal = {ARIST},
  volume = {25},
  number = {},
  pages = {209--260.},
  note = {},
  abstract = {},
  key = {},
  opturl = {},
}

@article{dunjko2016,
  title = {Quantum-Enhanced Machine Learning},
  author = {Dunjko, Vedran and Taylor, Jacob M. and Briegel, Hans J.},
  year = {2016},
  month = {Sep},
  journal = {Phys. Rev. Lett.},
  publisher = {American Physical Society},
  volume = {117},
  pages = {130501},
  abstract = {The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised, and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.},
  optdoi = {10.1103/PhysRevLett.117.130501},
  issue = {13},
  numpages = {6},
  opturl = {https://link.aps.org/doi/10.1103/PhysRevLett.117.130501},
}

@article{dunjko2017,
  title = {Advances in quantum reinforcement learning},
  author = {Vedran Dunjko and Jacob M. Taylor and Hans J. Briegel},
  year = {2017},
  month = {oct},
  journal = {2017 {IEEE} International Conference on Systems, Man, and Cybernetics ({SMC})},
  publisher = {{IEEE}},
  pages = {282--287},
  optdoi = {10.1109/smc.2017.8122616},
}

@article{dunjko2018,
  title = {Machine learning {\&} artificial intelligence in the quantum domain: a review of recent progress},
  author = {Vedran Dunjko and Hans J Briegel},
  year = {2018},
  month = {jun},
  journal = {Reports on Progress in Physics},
  publisher = {{IOP} Publishing},
  volume = {81},
  number = {7},
  pages = {074001},
  optdoi = {10.1088/1361-6633/aab406},
}

@article{esteva2019,
  title = {A guide to deep learning in healthcare},
  author = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
  year = {2019},
  journal = {Nature Medicine},
  volume = {25},
  number = {1},
  pages = {24--29},
  note = {},
  abstract = {Here we present deep-learning techniques for healthcare, centering our discussion on deep learning in computer vision, natural language processing, reinforcement learning, and generalized methods. We describe how these computational techniques can impact a few key areas of medicine and explore how to build end-to-end systems. Our discussion of computer vision focuses largely on medical imaging, and we describe the application of natural language processing to domains such as electronic health record data. Similarly, reinforcement learning is discussed in the context of robotic-assisted surgery, and generalized deep-learning methods for genomics are reviewed.},
  key = {},
  opturl = {https://doi.org/10.1038/s41591-018-0316-z},
}

@article{farhi2014,
  title = {A quantum approximate optimization algorithm},
  author = {Edward Farhi and Jeffrey Goldstone and Sam Gutmann},
  year = {2014},
  journal = {arXiv: 1411.4028},
}

@article{farhi2016,
  title = {Quantum Supremacy through the Quantum Approximate Optimization Algorithm},
  author = {Edward Farhi and Aram W Harrow},
  year = {2016},
  journal = {arXiv: 1602.07674},
}

@article{foesel2018,
  title = {Reinforcement Learning with Neural Networks for Quantum Feedback},
  author = {Thomas F\"osel and Petru Tighineanu and Talitha Weiss and Florian Marquardt},
  year = {2018},
  month = {sep},
  journal = {Physical Review X},
  publisher = {American Physical Society ({APS})},
  volume = {8},
  number = {3},
  pages = {031084},
  optdoi = {10.1103/physrevx.8.031084},
}

@article{ghahramani2015,
  title = {Probabilistic machine learning and artificial intelligence},
  author = {Ghahramani, Zoubin},
  year = {2015},
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {452--459},
  issn = {1476-4687},
  abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
  optdoi = {10.1038/nature14541},
  opturl = {https://doi.org/10.1038/nature14541},
}

@book{guido1998,
  title = {Visual Explorations in Finance},
  author = {Deboeck, Guido and  Kohonen, Teuvo},
  year = {1998},
  publisher = {Springer-Verlag London},
  address = {Reading, Massachusetts},
  optdoi = {10.1007/978-1-4471-3913-3},
}

@article{harrow2009,
  title = {Quantum Algorithm for Linear Systems of Equations},
  author = {Aram W. Harrow and Avinatan Hassidim and Seth Lloyd},
  year = {2009},
  month = {oct},
  journal = {Physical Review Letters},
  publisher = {American Physical Society ({APS})},
  volume = {103},
  number = {15},
  pages = {150502},
  optdoi = {10.1103/physrevlett.103.150502},
}

@article{huang2019,
  title = {Alibaba Cloud Quantum Development Platform: Applications to Quantum Algorithm Design},
  author = {Cupjin Huang and Mario Szegedy and Fang Zhang and Xun Gao and Jianxin Chen and Yaoyun Shi},
  year = {2019},
  journal = {arXiv: 1909.02559},
}

@misc{ibmq,
  title = {{IBM} {Q}uantum {E}xperience},
  url = {https://www.ibm.com/quantum-computing/},
}

@article{jeswal2019,
  title = {Recent Developments and Applications in Quantum Neural Network: A Review},
  author = {Jeswal, S. K. and Chakraverty, S.},
  year = {2019},
  journal = {Archives of Computational Methods in Engineering},
  volume = {26},
  number = {4},
  pages = {793--807},
  issn = {1886-1784},
  abstract = {Quantum neural network is a useful tool which has seen more development over the years mainly after twentieth century. Like artificial neural network (ANN), a novel, useful and applicable concept has been proposed recently which is known as quantum neural network (QNN). QNN has been developed combining the basics of ANN with quantum computation paradigm which is superior than the traditional ANN. QNN is being used in computer games, function approximation, handling big data etc. Algorithms of QNN are also used in modelling social networks, associative memory devices, and automated control systems etc. Different models of QNN has been proposed by different researchers throughout the world but systematic study of these models have not been done till date. Moreover, application of QNN may also be seen in some of the related research papers. As such, this paper includes different models which have been developed and further the implement of the same in various applications. In order to understand the powerfulness of QNN, few results and reasons are incorporated to show that these new models are more useful and efficient than traditional ANN.},
  optdoi = {10.1007/s11831-018-9269-0},
  opturl = {https://doi.org/10.1007/s11831-018-9269-0},
}

@article{jiang2017,
  title = {Near-optimal quantum circuit for Grover's unstructured search using a transverse field},
  author = {Zhang Jiang and Eleanor G. Rieffel and Zhihui Wang},
  year = {2017},
  month = {jun},
  journal = {Physical Review A},
  publisher = {American Physical Society ({APS})},
  volume = {95},
  number = {6},
  pages = {062317},
  optdoi = {10.1103/physreva.95.062317},
}

@article{jones2012,
  title = {The genomic basis of adaptive evolution in threespine sticklebacks},
  author = {Felicity C. Jones and and Manfred G. Grabherr and Yingguang Frank Chan and Pamela Russell and Evan Mauceli and Jeremy Johnson and Ross Swofford and Mono Pirun and Michael C. Zody and Simon White and Ewan Birney and Stephen Searle and Jeremy Schmutz and Jane Grimwood and Mark C. Dickson and Richard M. Myers and Craig T. Miller and Brian R. Summers and Anne K. Knecht and Shannon D. Brady and Haili Zhang and Alex A. Pollen and Timothy Howes and Chris Amemiya and Eric S. Lander and Federica Di Palma and Kerstin Lindblad-Toh and David M. Kingsley},
  year = {2012},
  month = {apr},
  journal = {Nature},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {484},
  number = {7392},
  pages = {55--61},
  optdoi = {10.1038/nature10944},
}

@misc{kaggle2014,
  title = {Kaggle: Higgs boson machine learning challenge.},
  author = {},
  year = {2014},
  volume = {},
  number = {},
  pages = {},
  url = {https://www.kaggle.com/c/higgs-boson},
  note = {},
  abstract = {},
  key = {},
  opturl = {},
}

% book?
@inproceedings{kamruzzaman2019,
  title = {Quantum Deep Learning Neural Networks},
  author = {Kamruzzaman, Abu and Alhwaiti, Yousef and Leider, Avery and Tappert, Charles C.},
  year = {2020},
  month = {feb},
  booktitle = {Knowledge Science, Engineering and Management},
  publisher = {Springer, Cham},
  pages = {299--311},
  isbn = {978-3-030-12385-7},
  editor = {Arai, Kohei and Bhatia, Rahul},
  optdoi = {10.1007/978-3-030-12385-7\_24},
}

@article{kandala2017,
  title = {Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets},
  author = {Abhinav Kandala and Antonio Mezzacapo and Kristan Temme and Maika Takita and Markus Brink and Jerry M. Chow and Jay M. Gambetta},
  year = {2017},
  month = {sep},
  journal = {Nature},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {549},
  number = {7671},
  pages = {242--246},
  optdoi = {10.1038/nature23879},
}

@article{kerenidis2018,
  title = {Quantum classification of the MNIST dataset via Slow Feature Analysis},
  author = {{Kerenidis}, Iordanis and {Luongo}, Alessandro},
  year = {2018},
  journal = {arXiv:1805.08837},
}

@article{key,
  title = {},
  author = {},
  year = {},
  journal = {},
  volume = {},
  number = {},
  pages = {},
  note = {},
  abstract = {},
  key = {},
  opturl = {},
}

@article{killoran2019,
  title = {Continuous-variable quantum neural networks},
  author = {Nathan Killoran and Thomas R. Bromley and Juan Miguel Arrazola and Maria Schuld and Nicol{\'{a}}s Quesada and Seth Lloyd},
  year = {2019},
  month = {oct},
  journal = {Physical Review Research},
  publisher = {American Physical Society ({APS})},
  volume = {1},
  number = {3},
  pages = {033063},
  optdoi = {10.1103/physrevresearch.1.033063},
}

% inproceedings
@article{kiviluotoa1996,
  title = {Topology preservation in self-organizing maps},
  author = {K. Kiviluoto},
  year = {1996},
  journal = {Proceedings of International Conference on Neural Networks (ICNN'96)},
  volume = {1},
  number = {},
  pages = {294--299},
}

@article{kohonen1990,
  title = {The self-organizing map},
  author = {T. {Kohonen}},
  year = {1990},
  month = {Sep.},
  journal = {Proceedings of the IEEE},
  volume = {78},
  number = {9},
  pages = {1464--1480},
  issn = {1558-2256},
  abstract = {The self-organized map, an architecture suggested for artificial neural networks, is explained by presenting simulation experiments and practical applications. The self-organizing map has the property of effectively creating spatially organized internal representations of various features of input signals and their abstractions. One result of this is that the self-organization process can discover semantic relationships in sentences. Brain maps, semantic maps, and early work on competitive learning are reviewed. The self-organizing map algorithm (an algorithm which order responses spatially) is reviewed, focusing on best matching cell selection and adaptation of the weight vectors. Suggestions for applying the self-organizing map algorithm, demonstrations of the ordering process, and an example of hierarchical clustering of data are presented. Fine tuning the map by learning vector quantization is addressed. The use of self-organized maps in practical speech recognition and a simulation experiment on semantic mapping are discussed.<>},
  optdoi = {10.1109/5.58325},
  keywords = {learning systems;neural nets;self-adjusting systems;speech recognition;self-organizing map;neural networks;semantic maps;competitive learning;clustering;learning vector;speech recognition;Biological neural networks;Artificial neural networks;Pattern recognition;Process control;Signal processing;Computer networks;Signal processing algorithms;Animals;Organizing;Speech recognition},
}

@article{kohonen1996,
  title = {Engineering applications of the self-organizing map},
  author = {T. Kohonen and E. Oja and O. Simula and A. Visa and J. Kangas},
  year = {1996},
  journal = {Proceedings of the {IEEE}},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {84},
  number = {10},
  pages = {1358--1384},
  optdoi = {10.1109/5.537105},
}

% inproceedings
@article{kohonen1997,
  title = {Exploration of very large databases by self-organizing maps},
  author = {T. Kohonen},
  year = {1997},
  journal = {Proceedings of International Conference on Neural Networks (ICNN'97)},
  volume = {1},
  number = {},
  pages = {PL1-PL6},
}

@article{kourtis2020,
  title = {Compiling Neural Networks for a Computational Memory Accelerator},
  author = {Kornilios Kourtis and Martino Dazzi and Nikolas Ioannou and Tobias Grosser and Abu Sebastian and Evangelos Eleftheriou},
  year = {2020},
  journal = {arxiv: 2003.04293},
}

@article{lanyon2010,
  title = {Towards quantum chemistry on a quantum computer},
  author = {B. P. Lanyon and J. D. Whitfield and G. G. Gillett and M. E. Goggin and M. P. Almeida and I. Kassal and J. D. Biamonte and M. Mohseni and B. J. Powell and M. Barbieri and A. Aspuru-Guzik and A. G. White},
  year = {2010},
  month = {jan},
  journal = {Nature Chemistry},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {2},
  number = {2},
  pages = {106--111},
  optdoi = {10.1038/nchem.483},
}

@article{lecun2015,
  title = {Deep learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  note = {},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  key = {},
  opturl = {https://doi.org/10.1038/nature14539},
}

@article{lewenstein1994,
  title = {Quantum Perceptrons},
  author = {M. Lewenstein},
  year = {1994},
  month = {dec},
  journal = {Journal of Modern Optics},
  publisher = {Informa {UK} Limited},
  volume = {41},
  number = {12},
  pages = {2491--2501},
  optdoi = {10.1080/09500349414552331},
}

@article{li2019,
  title = {Sublinear quantum algorithms for training linear and kernel-based classifiers},
  author = {{Li}, Tongyang and {Chakrabarti}, Shouvanik and {Wu}, Xiaodi},
  year = {2019},
  journal = {arXiv:1904.02276},
}

@article{liu2019,
  title = {Hybrid Quantum-Classical Convolutional Neural Networks},
  author = {Junhua Liu and Kwan Hui Lim and Kristin L. Wood and Wei Huang and Chu Guo and He-Liang Huang},
  year = {2019},
  journal = {arXiv: 1911.02998},
}

@article{lloyd2013,
  title = {Quantum algorithms for supervised and unsupervised machine learning},
  author = {Seth Lloyd and Masoud Mohseni and Patrick Rebentrost},
  year = {2013},
  journal = {arXiv: 1307.0411},
}

@article{lloyd2014,
  title = {Quantum principal component analysis},
  author = {Seth Lloyd and Masoud Mohseni and Patrick Rebentrost},
  year = {2014},
  month = {jul},
  journal = {Nature Physics},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {10},
  number = {9},
  pages = {631--633},
  optdoi = {10.1038/nphys3029},
}

@article{lu2019,
  title = {Quantum Adversarial Machine Learning},
  author = {Sirui Lu and Lu-Ming Duan and Dong-Ling Deng},
  year = {2019},
  journal = {arXiv: 2001.00030},
}

@article{marcus2018,
  title = {Deep Learning: A Critical Appraisal},
  author = {Marcus, Gary},
  year = {2018},
  journal = {arxiv: 1801.00631},
}

@article{mcclean2016,
  title = {The theory of variational hybrid quantum-classical algorithms},
  author = {Jarrod R McClean and Jonathan Romero and Ryan Babbush and Al{\'{a}}n Aspuru-Guzik},
  year = {2016},
  month = {feb},
  journal = {New Journal of Physics},
  publisher = {{IOP} Publishing},
  volume = {18},
  number = {2},
  pages = {023023},
  optdoi = {10.1088/1367-2630/18/2/023023},
}

@book{mctear2016,
  title = {The Conversational Interface},
  author = {Michael McTear and Zoraida Callejas and David Griol},
  year = {2016},
  publisher = {Springer International Publishing},
  optdoi = {10.1007/978-3-319-32967-3},
  opturl = {https://doi.org/10.1007\%2F978-3-319-32967-3},
}

@article{med0,
  title = {Targeting epigenetic regulators for cancer therapy: mechanisms and advances in clinical trials},
  author = {Cheng, Yuan and He, Cai and Wang, Manni and Ma, Xuelei and Mo, Fei and Yang, Shengyong and Han, Junhong and Wei, Xiawei},
  year = {2019},
  journal = {Signal Transduction and Targeted Therapy},
  volume = {4},
  number = {1},
  pages = {62},
  issn = {2059-3635},
  abstract = {Epigenetic alternations concern heritable yet reversible changes in histone or DNA modifications that regulate gene activity beyond the underlying sequence. Epigenetic dysregulation is often linked to human disease, notably cancer. With the development of various drugs targeting epigenetic regulators, epigenetic-targeted therapy has been applied in the treatment of hematological malignancies and has exhibited viable therapeutic potential for solid tumors in preclinical and clinical trials. In this review, we summarize the aberrant functions of enzymes in DNA methylation, histone acetylation and histone methylation during tumor progression and highlight the development of inhibitors of or drugs targeted at epigenetic enzymes.},
  optdoi = {10.1038/s41392-019-0095-0},
  opturl = {https://doi.org/10.1038/s41392-019-0095-0},
}

@article{med1,
  title = {Dietary patterns and cancer risk},
  author = {Steck, Susan E. and Murphy, E. Angela},
  year = {2020},
  journal = {Nature Reviews Cancer},
  volume = {20},
  pages = {125--138},
  issn = {1474-1768},
  abstract = {Over the past decade, the search for dietary factors on which to base cancer prevention guidelines has led to the rapid expansion of the field of dietary patterns and cancer. Multiple systematic reviews and meta-analyses have reported epidemiological associations between specific cancer types and both data-driven dietary patterns determined by empirical analyses and investigator-defined dietary indexes based on a predetermined set of dietary components. New developments, such as the use of metabolomics to identify objective biomarkers of dietary patterns and novel statistical techniques, could provide further insights into the links between diet and cancer risk. Although animal models of dietary patterns are limited, progress in this area could identify the potential mechanisms underlying the disease-specific associations observed in epidemiological studies. In this Review, we summarize the current state of the field, provide a critical appraisal of new developments and identify priority areas for future research. An underlying theme that emerges is that the effectiveness of different dietary pattern recommendations in reducing risk could depend on the type of cancer or on other risk factors such as family history, sex, age and other lifestyle factors or comorbidities as well as on metabolomic signatures or gut microbiota profiles.},
  optdoi = {10.1038/s41568-019-0227-4},
  opturl = {https://doi.org/10.1038/s41568-019-0227-4},
}

@article{med2,
  title = {Cancer's epigenetic drugs: where are they in the cancer medicines?},
  author = {Ghasemi, Sorayya},
  year = {2019},
  journal = {The Pharmacogenomics Journal},
  volume = {20},
  pages = {1473--1150},
  abstract = {Epigenetic modulation can affect the characteristics of cancers. Because it is likely to manipulate epigenetic genes, they can be considered as potential targets for cancer treatment. In this comprehensive study, epigenetic drugs are categorized according to anticancer mechanisms and phase of therapy. The relevant articles or databases were searched for epigenetic approaches to cancer therapy. Epigenetic drugs are divided according to their mechanisms and clinical phases that have been approved by the FDA or are undergoing evaluation phases. DNA methylation agents, chromatin remodelers specially HDACs, and noncoding RNAs especially microRNAs are the main epi-drugs for cancer. Despite many challenges, combination therapy using epi-drugs and routine therapies such as chemotherapy in various approaches have exhibited beneficial effects compared with each treatment alone. Cancer stem cell targeting and epigenetic editing have been confirmed as definitive pathways for cancer treatment. This paper reviewed the available epigenetic approaches to cancer therapy.},
  optdoi = {10.1038/s41397-019-0138-5},
  opturl = {https://doi.org/10.1038/s41397-019-0138-5},
}

@article{mishra2019,
  title = {Cancer Detection Using Quantum Neural Networks: A Demonstration on a Quantum Computer},
  author = {Nilima Mishra and Aradh Bisarya and Shubham Kumar and Bikash K. Behera and Sabyasachi Mukhopadhyay and Prasanta K. Panigrahi},
  year = {2019},
  journal = {arXiv: 1911.00504},
}

@article{mori2019,
  title = {Novel computational model of gastrula morphogenesis to identify spatial discriminator genes by self-organizing map (SOM) clustering},
  author = {Tomoya Mori and Haruka Takaoka and Junko Yamane and Cantas Alev and Wataru Fujibuchi},
  year = {2019},
  month = {aug},
  journal = {Scientific Reports},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {9},
  pages = {12597},
  issue = {12597},
  optdoi = {10.1038/s41598-019-49031-1},
}

@article{nautrup2019,
  title = {Optimizing Quantum Error Correction Codes with Reinforcement Learning},
  author = {Hendrik Poulsen Nautrup and Nicolas Delfosse and Vedran Dunjko and Hans J. Briegel and Nicolai Friis},
  year = {2019},
  month = {dec},
  journal = {Quantum},
  publisher = {Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
  volume = {3},
  pages = {215},
  optdoi = {10.22331/q-2019-12-16-215},
}

@article{pagano2019,
  title = {Quantum Approximate Optimization of the Long-Range Ising Model with a Trapped-Ion Quantum Simulator},
  author = {G. Pagano and A. Bapat and P. Becker and K. S. Collins and A. De and P. W. Hess and H. B. Kaplan and A. Kyprianidis and W. L. Tan and C. Baldwin and L. T. Brady and A. Deshpande and F. Liu and S. Jordan and A. V. Gorshkov and C. Monroe},
  year = {2019},
  journal = {arxiv: 1906.02700},
}

@article{paparo2014,
  title = {Quantum Speedup for Active Learning Agents},
  author = {Paparo, Giuseppe Davide and Dunjko, Vedran and Makmal, Adi and Martin-Delgado, Miguel Angel and Briegel, Hans J.},
  year = {2014},
  month = {Jul},
  journal = {Phys. Rev. X},
  publisher = {American Physical Society},
  volume = {4},
  pages = {031002},
  abstract = {Can quantum mechanics help us build intelligent learning agents? A defining signature of intelligent behavior is the capacity to learn from experience. However, a major bottleneck for agents to learn in real-life situations is the size and complexity of the corresponding task environment. Even in a moderately realistic environment, it may simply take too long to rationally respond to a given situation. If the environment is impatient, allowing only a certain time for a response, an agent may then be unable to cope with the situation and to learn at all. Here, we show that quantum physics can help and provide a quadratic speedup for active learning as a genuine problem of artificial intelligence. This result will be particularly relevant for applications involving complex task environments.},
  optdoi = {10.1103/PhysRevX.4.031002},
  issue = {3},
  numpages = {14},
  opturl = {https://link.aps.org/doi/10.1103/PhysRevX.4.031002},
}

@article{peruzzo2014,
  title = {A variational eigenvalue solver on a photonic quantum processor},
  author = {Alberto Peruzzo and Jarrod McClean and Peter Shadbolt and Man-Hong Yung and Xiao-Qi Zhou and Peter J. Love and Al{\'{a}}n Aspuru-Guzik and Jeremy L. O'Brien},
  year = {2014},
  month = {jul},
  journal = {Nature Communications},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {5},
  number = {1},
  pages = {4213},
  optdoi = {10.1038/ncomms5213},
}

@article{purushothaman1997,
  title = {Quantum neural networks ({QNNs}): inherently fuzzy feedforward neural networks},
  author = {G. Purushothaman and N.B. Karayiannis},
  year = {1997},
  month = {may},
  journal = {{IEEE} Transactions on Neural Networks},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {8},
  number = {3},
  pages = {679--693},
  optdoi = {10.1109/72.572106},
}

@article{pyrkov2019,
  title = {Solitonic Fixed Point Attractors in the Complex Ginzburg{\textendash}Landau Equation for Associative Memories},
  author = {Alexey N. Pyrkov and Tim Byrnes and Valentin V. Cherny},
  year = {2020},
  month = {dec},
  journal = {Symmetry},
  publisher = {{MDPI} {AG}},
  volume = {12},
  number = {1},
  pages = {24},
  optdoi = {10.3390/sym12010024},
}

@misc{qiskit,
  title = {Qiskit: An Open-source Framework for Quantum Computing},
  author = {H{\'e}ctor Abraham and AduOffei and Ismail Yunus Akhalwaya and Gadi Aleksandrowicz and Thomas Alexander and Gadi Alexandrowics and Eli Arbel and Abraham Asfaw and Carlos Azaustre and AzizNgoueya and Panagiotis Barkoutsos and George Barron and Luciano Bello and Yael Ben-Haim and Daniel Bevenius and Lev S. Bishop and Sorin Bolos and Samuel Bosch and Sergey Bravyi and David Bucher and Artemiy Burov and Fran Cabrera and Padraic Calpin and Lauren Capelluto and Jorge Carballo and Gin{\'e}s Carrascal and Adrian Chen and Chun-Fu Chen and Richard Chen and Jerry M. Chow and Christian Claus and Christian Clauss and Abigail J. Cross and Andrew W. Cross and Simon Cross and Juan Cruz-Benito and Chris Culver and Antonio D. C{\'o}rcoles-Gonzales and Sean Dague and Tareq El Dandachi and Matthieu Dartiailh and DavideFrr and Abd{\'o}n Rodr{\'\i}guez Davila and Anton Dekusar and Delton Ding and Jun optDoi and Eric Drechsler and Drew and Eugene Dumitrescu and Karel Dumon and Ivan Duran and Kareem EL-Safty and Eric Eastman and Pieter Eendebak and Daniel Egger and Mark Everitt and Paco Mart{\'\i}n Fern{\'a}ndez and Axel Hern{\'a}ndez Ferrera and Albert Frisch and Andreas Fuhrer and MELVIN GEORGE and Julien Gacon and Gadi and Borja Godoy Gago and Claudio Gambella and Jay M. Gambetta and Adhisha Gammanpila and Luis Garcia and Shelly Garion and Austin Gilliam and Juan Gomez-Mosquera and Salvador de la Puente Gonz{\'a}lez and Jesse Gorzinski and Ian Gould and Donny Greenberg and Dmitry Grinko and Wen Guan and John A. Gunnels and Mikael Haglund and Isabel Haide and Ikko Hamamura and Vojtech Havlicek and Joe Hellmers and {\L}ukasz Herok and Stefan Hillmich and Hiroshi Horii and Connor Howington and Shaohan Hu and Wei Hu and Haruki Imai and Takashi Imamichi and Kazuaki Ishizaki and Raban Iten and Toshinari Itoko and JamesSeaward and Ali Javadi and Ali Javadi-Abhari and Jessica and Kiran Johns and Tal Kachmann and Naoki Kanazawa and Kang-Bae and Anton Karazeev and Paul Kassebaum and Spencer King and Knabberjoe and Arseny Kovyrshin and Rajiv Krishnakumar and Vivek Krishnan and Kevin Krsulich and Gawel Kus and Ryan LaRose and Rapha{\"e}l Lambert and Joe Latone and Scott Lawrence and Dennis Liu and Peng Liu and Yunho Maeng and Aleksei Malyshev and Jakub Marecek and Manoel Marques and Dolph Mathews and Atsushi Matsuo and Douglas T. McClure and Cameron McGarry and David McKay and Dan McPherson and Srujan Meesala and Martin Mevissen and Antonio Mezzacapo and Rohit Midha and Zlatko Minev and Abby Mitchell and Nikolaj Moll and Michael Duane Mooring and Renier Morales and Niall Moran and MrF and Prakash Murali and Jan M{\"u}ggenburg and David Nadlinger and Ken Nakanishi and Giacomo Nannicini and Paul Nation and Edwin Navarro and Yehuda Naveh and Scott Wyman Neagle and Patrick Neuweiler and Pradeep Niroula and Hassi Norlen and Lee James O'Riordan and Oluwatobi Ogunbayo and Pauline Ollitrault and Steven Oud and Dan Padilha and Hanhee Paik and Simone Perriello and Anna Phan and Francesco Piro and Marco Pistoia and Alejandro Pozas-iKerstjens and Viktor Prutyanov and Daniel Puzzuoli and Jes{\'u}s P{\'e}rez and Quintiii and Rudy Raymond and Rafael Mart{\'\i}n-Cuevas Redondo and Max Reuter and Julia Rice and Diego M. Rodr{\'\i}guez and RohithKarur and Max Rossmannek and Mingi Ryu and Tharrmashastha SAPV and SamFerracin and Martin Sandberg and Hayk Sargsyan and Ninad Sathaye and Bruno Schmitt and Chris Schnabel and Zachary Schoenfeld and Travis L. Scholten and Eddie Schoute and Joachim Schwarm and Ismael Faro Sertage and Kanav Setia and Nathan Shammah and Yunong Shi and Adenilton Silva and Andrea Simonetto and Nick Singstock and Yukio Siraichi and Iskandar Sitdikov and Seyon Sivarajah and Magnus Berg Sletfjerding and John A. Smolin and Mathias Soeken and Igor Olegovich Sokolov and SooluThomas and Dominik Steenken and Matt Stypulkoski and Jack Suen and Shaojun Sun and Kevin J. Sung and Hitomi Takahashi and Ivano Tavernelli and Charles Taylor and Pete Taylour and Soolu Thomas and Mathieu Tillet and Maddy Tod and Enrique de la Torre and Kenso Trabing and Matthew Treinish and TrishaPe and Wes Turner and Yotam Vaknin and Carmen Recio Valcarce and Francois Varchon and Almudena Carrera Vazquez and Desiree Vogt-Lee and Christophe Vuillot and James Weaver and Rafal Wieczorek and Jonathan A. Wildstrom and Robert Wille and Erick Winston and Jack J. Woehr and Stefan Woerner and Ryan Woo and Christopher J. Wood and Ryan Wood and Stephen Wood and Steve Wood and James Wootton and Daniyar Yeralin and Richard Young and Jessie Yu and Christopher Zachow and Laura Zdanski and Christa Zoufal and Zoufalc and a-matsuo and adekusar-drl and azulehner and bcamorrison and brandhsn and chlorophyll-zz and dan1pal and dime10 and drholmie and elfrocampeador and faisaldebouni and fanizzamarco and gadial and gruu and jliu45 and kanejess and klinvill and kurarrr and lerongil and ma5x and merav-aharoni and michelle4654 and ordmoj and sethmerkel and strickroman and sumitpuri and tigerjack and toural and vvilpas and welien and willhbang and yang.luh and yelojakit and yotamvakninibm},
  year = {2019},
  doi = {10.5281/zenodo.2562110},
}

@article{qml0,
  title = {Quantum computing and the brain: quantum nets, dessins d\'{}enfants and neural networks},
  author = {{Asselmeyer-Maluga, Torsten}},
  year = {2019},
  journal = {EPJ Web Conf.},
  volume = {198},
  pages = {14},
  abstract = {In this paper, we will discuss a formal link between neural networks and quantum computing. For that purpose we will present a simple model for the description of the neural network by forming sub-graphs of the whole network with the same or a similar state. We will describe the interaction between these areas by closed loops, the feedback loops. The change of the graph is given by the deformations of the loops. This fact can be mathematically formalized by the fundamental group of the graph. Furthermore the neuron has two basic states \vert{}0〉 (ground state) and \vert{}1〉 (excited state). The whole state of an area of neurons is the linear combination of the two basic state with complex coefficients representing the signals (with 3 Parameters: amplitude, frequency and phase) along the neurons. If something changed in this area, we need a transformation which will preserve this general form of a state (mathematically, this transformation must be an element of the group S L(2; C)). The same argumentation must be true for the feedback loops, i.e. a general transformation of states along the feedback loops is an assignment of this loop to an element of the transformation group. Then it can be shown that the set of all signals forms a manifold (character variety) and all properties of the network must be encoded in this manifold. In the paper, we will discuss how to interpret learning and intuition in this model. Using the Morgan-Shalen compactification, the limit for signals with large amplitude can be analyzed by using quasi-Fuchsian groups as represented by dessins d'enfants (graphs to analyze Riemannian surfaces). As shown by Planat and collaborators, these dessins d'enfants are a direct bridge to (topological) quantum computing with permutation groups. The normalization of the signal reduces to the group S U(2) and the whole model to a quantum network. Then we have a direct connection to quantum circuits. This network can be transformed into operations on tensor networks. Formally we will obtain a link between machine learning and Quantum computing.},
  optdoi = {10.1051/epjconf/201919800014},
  opturl = {https://doi.org/10.1051/epjconf/201919800014},
}

@article{qml1,
  title = {A Simple Quantum Neural Net with a Periodic Activation Function},
  author = {A. {Daskin}},
  year = {2018},
  month = {Oct},
  journal = {2018 IEEE International Conference on Systems, Man, and Cybernetics},
  volume = {},
  number = {},
  pages = {2887--2891},
  issn = {1062-922X},
  abstract = {In this paper, we propose a simple neural net that requires only O(n log\_2k) number of qubits and O(nk) quantum gates: Here, N is the number of input parameters, and k is the number of weights applied to these parameters in the proposed neural net. We describe the network in terms of a quantum circuit, and then draw its equivalent classical neural net which involves O(k^n) nodes in the hidden layer. Then, we show that the network uses a periodic activation function of cosine values of the linear combinations of the inputs and weights. The backpropagation is described through the gradient descent, and then iris and breast cancer datasets are used for the simulations. The numerical results indicate the network can be used in machine learning problems and it may provide exponential speedup over the same structured classical neural net.},
  optdoi = {10.1109/SMC.2018.00491},
  keywords = {backpropagation;computational complexity;gradient methods;neural nets;quantum gates;quantum gates;cosine values;gradient descent;machine learning problems;equivalent classical neural net;quantum circuit;input parameters;qubits;periodic activation function;simple quantum neural net;Qubit;Logic gates;Biological neural networks;Machine learning;Computational modeling;quantum machine learning;quantum neural networks},
}

@article{qml2,
  title = {Image Classification Using Quantum Inference on the D-Wave 2X},
  author = {Nguyen, Nga T.T. and Kenyon, Garrett T.},
  year = {2018},
  month = {Nov},
  journal = {2018 IEEE International Conference on Rebooting Computing},
  publisher = {IEEE},
  pages = {2887--2891},
  isbn = {9781538691700},
  abstract = {We use a quantum annealing D-Wave 2X computer to obtain solutions to NP-hard sparse coding problems. To reduce the dimensionality of the sparse coding problem to fit on the quantum D-Wave 2X hardware, we passed downsampled MNIST images through a bottleneck autoencoder. To establish a benchmark for classification performance on this reduced dimensional data set, we used an AlexNet-like architecture implemented in TensorFlow, obtaining a classification score of 94.54\pm{}0.7\%. As a control, we showed that the same AlexNet-like architecture produced near-state-of-the-art classification performance (\sim{}99\%) on the original MNIST images. To obtain a set of optimized features for inferring sparse representations of the reduced dimensional MNIST dataset, we imprinted on a random set of 47 image patches followed by an off-line unsupervised learning algorithm using stochastic gradient descent to optimize for sparse coding. Our single-layer of sparse coding matched the stride and patch size of the first convolutional layer of the AlexNet-like deep neural network and contained 47 fully-connected features, 47 being the maximum number of dictionary elements that could be embedded onto the D-Wave 2X hardware. Recent work suggests that the optimal level of sparsity corresponds to a critical value of the trade-off parameter associated with a putative second order phase transition, an observation supported by a free energy analysis of D-Wave energy states. When the sparse representations inferred by the D-Wave 2X were passed to a linear support vector machine, we obtained a classification score of 95.68\%. Thus, on this problem, we find that a single-layer of quantum inference is able to outperform a standard deep neural network architecture.},
  optdoi = {10.1109/icrc.2018.8638596},
  opturl = {http://dx.doi.org/10.1109/ICRC.2018.8638596},
}

@article{radovic2018,
  title = {Machine learning at the energy and intensity frontiers of particle physics},
  author = {Radovic, Alexander and Williams, Mike and Rousseau, David and Kagan, Michael and Bonacorsi, Daniele and Himmel, Alexander and Aurisano, Adam and Terao, Kazuhiro and Wongjirad, Taritree},
  year = {2018},
  journal = {Nature},
  volume = {560},
  number = {7716},
  pages = {41--48},
  issn = {1476-4687},
  abstract = {Our knowledge of the fundamental particles of nature and their interactions is summarized by the standard model of particle physics. Advancing our understanding in this field has required experiments that operate at ever higher energies and intensities, which produce extremely large and information-rich data samples. The use of machine-learning techniques is revolutionizing how we interpret these data samples, greatly increasing the discovery potential of present and future experiments. Here we summarize the challenges and opportunities that come with the use of machine learning at the frontiers of particle physics.},
  optdoi = {10.1038/s41586-018-0361-2},
  opturl = {https://doi.org/10.1038/s41586-018-0361-2},
}

@article{rebentrost2014,
  title = {Quantum Support Vector Machine for Big Data Classification},
  author = {Patrick Rebentrost and Masoud Mohseni and Seth Lloyd},
  year = {2014},
  month = {sep},
  journal = {Physical Review Letters},
  publisher = {American Physical Society ({APS})},
  volume = {113},
  number = {13},
  pages = {130503},
  optdoi = {10.1103/physrevlett.113.130503},
}

@article{rebentrost2018,
  title = {Quantum Hopfield neural network},
  author = {Rebentrost, Patrick and Bromley, Thomas R. and Weedbrook, Christian and Lloyd, Seth},
  year = {2018},
  month = {Oct},
  journal = {Phys. Rev. A},
  publisher = {American Physical Society},
  volume = {98},
  pages = {042308},
  optdoi = {10.1103/PhysRevA.98.042308},
  issue = {4},
  numpages = {11},
  opturl = {https://link.aps.org/doi/10.1103/PhysRevA.98.042308},
}

@article{liao2021quadratic,
  title = {Quadratic quantum speedup for perceptron training},
  author = {Liao, Pengcheng and Sanders, Barry C and Byrnes, Tim},
  year = {2024},
  month = {Oct},
  journal = {Phys. Rev. A},
  publisher = {American Physical Society},
  volume = {110},
  pages = {062412},
  issue = {4},
}

% inproceedings
@article{santana2017,
  title = {An alternative approach for binary and categorical self-organizing maps},
  author = {Alessandra Santana and Alessandra Morais and Marcos G. Quiles},
  year = {2017},
  month = {may},
  journal = {2017 International Joint Conference on Neural Networks ({IJCNN})},
  publisher = {{IEEE}},
  pages = {2604--2610},
  optdoi = {10.1109/ijcnn.2017.7966174},
  opturl = {https://doi.org/10.1109\%2Fijcnn.2017.7966174},
}

@article{schuld2014,
  title = {An introduction to quantum machine learning},
  author = {Maria Schuld and Ilya Sinayskiy and Francesco Petruccione},
  year = {2014},
  month = {oct},
  journal = {Contemporary Physics},
  publisher = {Informa {UK} Limited},
  volume = {56},
  number = {2},
  pages = {172--185},
  optdoi = {10.1080/00107514.2014.964942},
}

@article{schuld2014b,
  title = {The quest for a Quantum Neural Network},
  author = {Maria Schuld and Ilya Sinayskiy and Francesco Petruccione},
  year = {2014},
  month = {aug},
  journal = {Quantum Information Processing},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {13},
  number = {11},
  pages = {2567--2586},
  optdoi = {10.1007/s11128-014-0809-8},
}

@book{schwab2017,
  title = {The Fourth Industrial Revolution},
  author = {Schwab, Klaus},
  year = {2017},
  publisher = {Currency Press},
  address = {Redfern, New South Wales},
}

@article{solan2001,
  title = {Similarity in Perception: A Window to Brain Organization},
  author = {Zach Solan and Eytan Ruppin},
  year = {2001},
  month = {jan},
  journal = {Journal of Cognitive Neuroscience},
  publisher = {{MIT} Press - Journals},
  volume = {13},
  number = {1},
  pages = {18--30},
  optdoi = {10.1162/089892901564144},
}

@article{sze2017,
  title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  author = {Vivienne Sze and Yu-Hsin Chen and Tien-Ju Yang and Joel S. Emer},
  year = {2017},
  month = {dec},
  journal = {Proceedings of the {IEEE}},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {105},
  number = {12},
  pages = {2295--2329},
  optdoi = {10.1109/jproc.2017.2761740},
}

@article{mohseni2023deep,
  title = {Deep recurrent networks predicting the gap evolution in adiabatic quantum computing},
  author = {Mohseni, Naeimeh and Navarrete-Benlloch, Carlos and Byrnes, Tim and Marquardt, Florian},
  year = {2023},
  journal = {Quantum},
  publisher = {Verein zur F{\"o}rderung des Open Access Publizierens in den Quantenwissenschaften},
  volume = {7},
  pages = {1039},
}

@article{mohseni2024deep,
  title = {Deep learning of many-body observables and quantum information scrambling},
  author = {Mohseni, Naeimeh and Shi, Junheng and Byrnes, Tim and Hartmann, Michael J},
  year = {2024},
  journal = {Quantum},
  publisher = {Verein zur F{\"o}rderung des Open Access Publizierens in den Quantenwissenschaften},
  volume = {8},
  pages = {1417},
}

@article{tacchino2019,
  title = {Quantum implementation of an artificial feed-forward neural network},
  author = {Francesco Tacchino and Panagiotis Barkoutsos and Chiara Macchiavello and Ivano Tavernelli and Dario Gerace and Daniele Bajoni},
  year = {2019},
  journal = {arXiv: 1912.12486},
}

@article{trugenberger2001,
  title = {Probabilistic Quantum Memories},
  author = {C. A. Trugenberger},
  year = {2001},
  month = {jul},
  journal = {Physical Review Letters},
  publisher = {American Physical Society ({APS})},
  volume = {87},
  number = {6},
  pages = {067901},
  optdoi = {10.1103/physrevlett.87.067901},
}

@article{tyrsa2017,
  title = {A Review of Deep Learning Methods and Applications for Unmanned Aerial Vehicles},
  author = {Tyrsa, Vera and Carrio, Adrian and Sampedro, Carlos and Rodriguez-Ramos, Alejandro and Campoy, Pascual},
  year = {2017},
  journal = {Journal of Sensors},
  volume = {2017},
  number = {},
  pages = {3296874},
  note = {},
  abstract = {Deep learning is recently showing outstanding results for solving a wide variety of robotic tasks in the areas of perception, planning, localization, and control. Its excellent capabilities for learning representations from the complex data acquired in real environments make it extremely suitable for many kinds of autonomous robotic applications. In parallel, Unmanned Aerial Vehicles (UAVs) are currently being extensively applied for several types of civilian tasks in applications going from security, surveillance, and disaster rescue to parcel delivery or warehouse management. In this paper, a thorough review has been performed on recent reported uses and applications of deep learning for UAVs, including the most relevant developments as well as their performances and limitations. In addition, a detailed explanation of the main deep learning techniques is provided. We conclude with a description of the main challenges for the application of deep learning for UAV-based solutions.},
  key = {},
  opturl = {https://doi.org/10.1155/2017/3296874},
}

@book{nielsen2010quantum,
  title = {Quantum computation and quantum information},
  author = {Nielsen, Michael A and Chuang, Isaac L},
  year = {2010},
  publisher = {Cambridge university press},
}

@book{byrnes2021quantum,
  title = {Quantum atom optics: Theory and applications to quantum technology},
  author = {Byrnes, Tim and Ilo-Okeke, Ebubechukwu O},
  year = {2021},
  publisher = {Cambridge university press},
}

@article{verdon2019,
  title = {Quantum Graph Neural Networks},
  author = {Guillaume Verdon and Trevor McCourt and Enxhell Luzhnica and Vikash Singh and Stefan Leichenauer and Jack Hidary},
  year = {2019},
  journal = {arXiv: 1909.12264},
}

@article{vesanto2000,
  title = {Clustering of the self-organizing map},
  author = {J. Vesanto and E. Alhoniemi},
  year = {2000},
  month = {may},
  journal = {{IEEE} Transactions on Neural Networks},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {11},
  number = {3},
  pages = {586--600},
  optdoi = {10.1109/72.846731},
}

@article{vilibic2016,
  title = {Self-Organizing Maps-based ocean currents forecasting system},
  author = {Vilibic, Ivica and Sepic, Jadranka and Mihanovic, Hrvoje and Kalinic, Hrvoje and Cosoli, Simone and Janekovic, Ivica and Zagar, Nedjeljka and Jesenko, Blaz and Tudor, Martina and Dadic, Vlado and Ivankovic, Damir},
  year = {2016},
  journal = {Scientific Reports},
  volume = {6},
  number = {1},
  pages = {22924},
  issn = {2045-2322},
  abstract = {An ocean surface currents forecasting system, based on a Self-Organizing Maps (SOM) neural network algorithm, high-frequency (HF) ocean radar measurements and numerical weather prediction (NWP) products, has been developed for a coastal area of the northern Adriatic and compared with operational ROMS-derived surface currents. The two systems differ significantly in architecture and algorithms, being based on either unsupervised learning techniques or ocean physics. To compare performance of the two methods, their forecasting skills were tested on independent datasets. The SOM-based forecasting system has a slightly better forecasting skill, especially during strong wind conditions, with potential for further improvement when data sets of higher quality and longer duration are used for training.},
  optdoi = {10.1038/srep22924},
  opturl = {https://doi.org/10.1038/srep22924},
}

@article{vinci2019,
  title = {A Path Towards Quantum Advantage in Training Deep Generative Models with Quantum Annealers},
  author = {Walter Vinci and Lorenzo Buffoni and Hossein Sadeghi and Amir Khoshaman and Evgeny Andriyash and Mohammad H. Amin},
  year = {2019},
  journal = {arXiv: 1912.02119},
}

@article{wang2018,
  title = {Quantum approximate optimization algorithm for {MaxCut}: A fermionic view},
  author = {Zhihui Wang and Stuart Hadfield and Zhang Jiang and Eleanor G. Rieffel},
  year = {2018},
  month = {feb},
  journal = {Physical Review A},
  publisher = {American Physical Society ({APS})},
  volume = {97},
  number = {2},
  pages = {022304},
  optdoi = {10.1103/physreva.97.022304},
}

@article{wecker2016,
  title = {Training a quantum optimizer},
  author = {Dave Wecker and Matthew B. Hastings and Matthias Troyer},
  year = {2016},
  month = {aug},
  journal = {Physical Review A},
  publisher = {American Physical Society ({APS})},
  volume = {94},
  number = {2},
  pages = {022309},
  optdoi = {10.1103/physreva.94.022309},
}

@inproceedings{weikang2016,
  title = {BOWL: Bag of Word Clusters Text Representation Using Word Embeddings},
  author = {Rui, Weikang and Xing, Kai and Jia, Yawei},
  year = {2016},
  booktitle = {Knowledge Science, Engineering and Management},
  publisher = {Springer International Publishing},
  address = {Cham},
  pages = {3--14},
  isbn = {978-3-319-47650-6},
  editor = {Lehner, Franz and Fteimi, Nora},
  abstract = {The text representation is fundamental for text mining and information retrieval. The Bag Of Words (BOW) and its variants (e.g. TF-IDF) are very basic text representation methods. Although the BOW and TF-IDF are simple and perform well in tasks like classification and clustering, its representation efficiency is extremely low. Besides, word level semantic similarity is not captured which results failing to capture text level similarity in many situations. In this paper, we propose a straightforward Bag Of Word cLusters (BOWL) representation for texts in a higher level, much lower dimensional space. We exploit the word embeddings to group semantically close words and consider them as a whole. The word embeddings are trained on a large corpus and incorporate extensive knowledge. We demonstrate on three benchmark datasets and two tasks, that BOWL representation shows significant advantages in terms of representation accuracy and efficiency.},
}

@article{wiebe2012,
  title = {Quantum Algorithm for Data Fitting},
  author = {Nathan Wiebe and Daniel Braun and Seth Lloyd},
  year = {2012},
  month = {aug},
  journal = {Physical Review Letters},
  publisher = {American Physical Society ({APS})},
  volume = {109},
  number = {5},
  pages = {050505},
  optdoi = {10.1103/physrevlett.109.050505},
}

@article{zhao2019,
  title = {Bayesian deep learning on a quantum computer},
  author = {Zhikuan Zhao and Alejandro Pozas-Kerstjens and Patrick Rebentrost and Peter Wittek},
  year = {2019},
  month = {may},
  journal = {Quantum Machine Intelligence},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {1},
  number = {1-2},
  pages = {41--51},
  optdoi = {10.1007/s42484-019-00004-7},
}

@article{zhu2018,
  title = {Biologically Inspired Self-Organizing Map Applied to Task Assignment and Path Planning of an {AUV} System},
  author = {Daqi Zhu and Xiang Cao and Bing Sun and Chaomin Luo},
  year = {2018},
  month = {jun},
  journal = {{IEEE} Transactions on Cognitive and Developmental Systems},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {10},
  number = {2},
  pages = {304--313},
  optdoi = {10.1109/tcds.2017.2727678},
}

@article{byrnes2018,
  title = {Generalized Grover's Algorithm for Multiple Phase Inversion States},
  author = {Tim Byrnes and Gary Forster and Louis Tessler},
  year = {2018},
  journal = {Physical Review Letters},
  publisher = {American Physical Society ({APS})},
  volume = {120},
  pages = {060501},
}

@article{preskill2018,
  title = {Quantum Computing in the {NISQ} era and beyond},
  author = {John Preskill},
  year = {2018},
  month = {aug},
  journal = {Quantum},
  publisher = {Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
  volume = {2},
  pages = {79},
  optdoi = {10.22331/q-2018-08-06-79},
}

@misc{NobelPrizePhysics,
  title = {The {Nobel} {Prize} in {Physics}},
  year = {2024},
  journal = {NobelPrize.org},
  url = {https://www.nobelprize.org/prizes/physics/2024/press-release/},
  urldate = {2024-12-04},
  abstract = {The Nobel Prize in Physics 2024 was awarded jointly to John J. Hopfield and Geoffrey E. Hinton "for foundational discoveries and inventions that enable machine learning with artificial neural networks"},
  file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\584WQJ4Q\\press-release.html:text/html},
}

@article{zardiniQuantumKnearestNeighbors2024,
  title = {A quantum k-nearest neighbors algorithm based on the {Euclidean} distance estimation},
  author = {Zardini, Enrico and Blanzieri, Enrico and Pastorello, Davide},
  year = {2024},
  month = apr,
  journal = {Quantum Machine Intelligence},
  volume = {6},
  number = {1},
  pages = {23},
  doi = {10.1007/s42484-024-00155-2},
  issn = {2524-4914},
  url = {https://doi.org/10.1007/s42484-024-00155-2},
  urldate = {2024-12-08},
  abstract = {The k-nearest neighbors (k-NN) is a basic machine learning (ML) algorithm, and several quantum versions of it, employing different distance metrics, have been presented in the last few years. Although the Euclidean distance is one of the most widely used distance metrics in ML, it has not received much consideration in the development of these quantum variants. In this article, a novel quantum k-NN algorithm based on the Euclidean distance is introduced. Specifically, the algorithm is characterized by a quantum encoding requiring a low number of qubits and a simple quantum circuit not involving oracles, aspects that favor its realization. In addition to the mathematical formulation and some complexity observations, a detailed empirical evaluation with simulations is presented. In particular, the results have shown the correctness of the formulation, a drop in the performance of the algorithm when the number of measurements is limited, the competitiveness with respect to some classical baseline methods in the ideal case, and the possibility of improving the performance by increasing the number of measurements.},
  keywords = {Artificial Intelligence, Euclidean distance, k-Nearest neighbors, Quantum computing, Quantum Computing, Quantum machine learning},
  file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\7QRP79ZS\\Zardini и др. - 2024 - A quantum k-nearest neighbors algorithm based on the Euclidean distance estimation.pdf:application/pdf},
}

@article{rathQuantumDataEncoding2024,
  title = {Quantum Data Encoding: A Comparative Analysis of Classical-to-Quantum Mapping Techniques and Their Impact on Machine Learning Accuracy},
  shorttitle = {Quantum Data Encoding},
  author = {Rath, Minati and Date, Hema},
  year = {2024},
  month = dec,
  journal = {EPJ Quantum Technology},
  publisher = {SpringerOpen},
  volume = {11},
  number = {1},
  pages = {1--22},
  doi = {10.1140/epjqt/s40507-024-00285-3},
  issn = {2196-0763},
  urldate = {2024-12-08},
  copyright = {2024 The Author(s)},
  abstract = {This study explores the integration of quantum data embedding techniques into classical machine learning (ML) algorithms; to assess performance enhancements and computational implications across a spectrum of models. We explored various classical-to-quantum mapping methods; ranging from basis encoding and angle encoding to amplitude encoding; for encoding classical data. We conducted an extensive empirical study encompassing popular ML algorithms, including Logistic Regression, K-Nearest Neighbors, Support Vector Machines, and ensemble methods like Random Forest, LightGBM, AdaBoost, and CatBoost. Our findings reveal that quantum data embedding contributes to improved classification accuracy and F1 scores, particularly notable in models that inherently benefit from enhanced feature representation. We observed nuanced effects on running time, with low-complexity models exhibiting moderate increases and more computationally intensive models experiencing discernible changes. Notably, ensemble methods demonstrated a favorable balance between performance gains and computational overhead. This study underscores the potential of quantum data embedding to enhance classical ML models and emphasizes the importance of weighing performance improvements against computational costs. Future research may involve refining quantum encoding processes to optimize computational efficiency and explore scalability for real-world applications. Our work contributes to the growing body of knowledge on the intersection of quantum computing and classical machine learning, offering insights for researchers and practitioners seeking to harness the advantages of quantum-inspired techniques in practical scenarios.},
  langid = {english},
  file = {C:\Users\alex\Zotero\storage\525XATVX\Rath и Date - 2024 - Quantum data encoding a comparative analysis of classical-to-quantum mapping techniques and their i.pdf},
}

@article{yuQuantumAlgorithmsSimilarity2020,
  title = {Quantum {Algorithms} for {Similarity} {Measurement} {Based} on {Euclidean} {Distance}},
  author = {Yu, Kai and Guo, Gong-De and Li, Jing and Lin, Song},
  year = {2020},
  month = oct,
  journal = {International Journal of Theoretical Physics},
  volume = {59},
  number = {10},
  pages = {3134--3144},
  doi = {10.1007/s10773-020-04567-1},
  issn = {1572-9575},
  url = {https://doi.org/10.1007/s10773-020-04567-1},
  urldate = {2024-12-08},
  abstract = {Similarity measurement is a fundamental problem that arise both on its own and as a key subroutine in more complex tasks, such as machine learning. However, in classical algorithms, the time used to similarity measurement usually increases exponentially as the amount of data and the number of data dimensions increase. In this paper, we presented three quantum algorithms based on Euclidean distance to measure the similarity between data sets. In the proposed algorithms, some special unitary operations are utilized to construct imperative quantum states from quantum random access memory. Then, a badly needed result for estimating the similarity between data sets, can be got by performing projective measurements. Furthermore, it is shown that these algorithms can achieve the exponential acceleration of the classical algorithm in the quantity or the dimension of data.},
  keywords = {Euclidean distance, Quantum algorithm, Quantum Computing, Quantum machine learning, Similarity measurement},
}

@misc{NobelPrizeChemistry,
  title = {The {Nobel} {Prize} in {Chemistry}},
  year = {2024},
  journal = {NobelPrize.org},
  url = {https://www.nobelprize.org/prizes/chemistry/2024/press-release/},
  urldate = {2024-12-04},
  abstract = {The Nobel Prize in Chemistry 2024 was divided, one half awarded to David Baker "for computational protein design", the other half jointly to Demis Hassabis and John Jumper "for protein structure prediction"},
  file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\5F96XNJF\\press-release.html:text/html},
}

@article{caroGeneralizationQuantumMachine2022,
  title = {Generalization in quantum machine learning from few training data},
  author = {Caro, Matthias C. and Huang, Hsin-Yuan and Cerezo, M. and Sharma, Kunal and Sornborger, Andrew and Cincio, Lukasz and Coles, Patrick J.},
  year = {2022},
  month = aug,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {4919},
  doi = {10.1038/s41467-022-32550-3},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/s41467-022-32550-3},
  urldate = {2024-12-04},
  copyright = {2022 The Author(s)},
  note = {Publisher: Nature Publishing Group},
  abstract = {Modern quantum machine learning (QML) methods involve variationally optimizing a parameterized quantum circuit on a training data set, and subsequently making predictions on a testing data set (i.e., generalizing). In this work, we provide a comprehensive study of generalization performance in QML after training on a limited number N of training data points. We show that the generalization error of a quantum machine learning model with T trainable gates scales at worst as \$\${\textbackslash}sqrt\{T/N\}\$\$. When only K\hspace{0.167em}\ll{}\hspace{0.167em}T gates have undergone substantial change in the optimization process, we prove that the generalization error improves to \$\${\textbackslash}sqrt\{K/N\}\$\$. Our results imply that the compiling of unitaries into a polynomial number of native gates, a crucial application for the quantum computing industry that typically uses exponential-size training data, can be sped up significantly. We also show that classification of quantum states across a phase transition with a quantum convolutional neural network requires only a very small training data set. Other potential applications include learning quantum error correcting codes or quantum dynamical simulation. Our work injects new hope into the field of QML, as good generalization is guaranteed from few training data.},
  keywords = {Computer science, Quantum information},
  file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\DBH9II24\\Caro и др. - 2022 - Generalization in quantum machine learning from few training data.pdf:application/pdf},
}

@article{anschuetzQuantumVariationalAlgorithms2022,
  title = {Quantum variational algorithms are swamped with traps},
  author = {Anschuetz, Eric R. and Kiani, Bobak T.},
  year = {2022},
  month = dec,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {7760},
  doi = {10.1038/s41467-022-35364-5},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/s41467-022-35364-5},
  urldate = {2024-12-04},
  copyright = {2022 The Author(s)},
  note = {Publisher: Nature Publishing Group},
  abstract = {One of the most important properties of classical neural networks is how surprisingly trainable they are, though their training algorithms typically rely on optimizing complicated, nonconvex loss functions. Previous results have shown that unlike the case in classical neural networks, variational quantum models are often not trainable. The most studied phenomenon is the onset of barren plateaus in the training landscape of these quantum models, typically when the models are very deep. This focus on barren plateaus has made the phenomenon almost synonymous with the trainability of quantum models. Here, we show that barren plateaus are only a part of the story. We prove that a wide class of variational quantum models--which are shallow, and exhibit no barren plateaus--have only a superpolynomially small fraction of local minima within any constant energy from the global minimum, rendering these models untrainable if no good initial guess of the optimal parameters is known. We also study the trainability of variational quantum algorithms from a statistical query framework, and show that noisy optimization of a wide variety of quantum models is impossible with a sub-exponential number of queries. Finally, we numerically confirm our results on a variety of problem instances. Though we exclude a wide variety of quantum algorithms here, we give reason for optimism for certain classes of variational algorithms and discuss potential ways forward in showing the practical utility of such algorithms.},
  keywords = {Quantum information, Theoretical physics},
  file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\QKJB7CYS\\Anschuetz и Kiani - 2022 - Quantum variational algorithms are swamped with traps.pdf:application/pdf},
}

@article{abbasPowerQuantumNeural2021,
  title = {The power of quantum neural networks},
  author = {Abbas, Amira and Sutter, David and Zoufal, Christa and Lucchi, Aurelien and Figalli, Alessio and Woerner, Stefan},
  year = {2021},
  month = jun,
  journal = {Nature Computational Science},
  volume = {1},
  number = {6},
  pages = {403--409},
  doi = {10.1038/s43588-021-00084-1},
  issn = {2662-8457},
  url = {https://www.nature.com/articles/s43588-021-00084-1},
  urldate = {2024-12-04},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  note = {Publisher: Nature Publishing Group},
  abstract = {It is unknown whether near-term quantum computers are advantageous for machine learning tasks. In this work we address this question by trying to understand how powerful and trainable quantum machine learning models are in relation to popular classical neural networks. We propose the effective dimension--a measure that captures these qualities--and prove that it can be used to assess any statistical model's ability to generalize on new data. Crucially, the effective dimension is a data-dependent measure that depends on the Fisher information, which allows us to gauge the ability of a model to train. We demonstrate numerically that a class of quantum neural networks is able to achieve a considerably better effective dimension than comparable feedforward networks and train faster, suggesting an advantage for quantum machine learning, which we verify on real quantum hardware.},
  keywords = {Computer science, Quantum information},
  file = {Отправленная версия:C\:\\Users\\alex\\Zotero\\storage\\GFTY8FWM\\Abbas и др. - 2021 - The power of quantum neural networks.pdf:application/pdf},
}

@misc{IBMRoadmapQuantumcentric,
  title = {{IBM} roadmap to quantum-centric supercomputers ({Updated} 2024) {\textbar} {IBM} {Quantum} {Computing} {Blog}},
  url = {https://www.ibm.com/quantum/blog/ibm-quantum-roadmap-2025},
  urldate = {2024-12-04},
  abstract = {The updated IBM Quantum roadmap: weaving quantum processors, CPUs, and GPUs into a compute fabric to solve problems beyond the scope of classical resources.},
  file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\T3IIVYAY\\ibm-quantum-roadmap-2025.html:text/html},
}

@misc{acharyaQuantumErrorCorrection2024,
  title = {Quantum error correction below the surface code threshold},
  author = {Acharya, Rajeev and Aghababaie-Beni, Laleh and Aleiner, Igor and Andersen, Trond I. and Ansmann, Markus and Arute, Frank and Arya, Kunal and Asfaw, Abraham and Astrakhantsev, Nikita and Atalaya, Juan and Babbush, Ryan and Bacon, Dave and Ballard, Brian and Bardin, Joseph C. and Bausch, Johannes and Bengtsson, Andreas and Bilmes, Alexander and Blackwell, Sam and Boixo, Sergio and Bortoli, Gina and Bourassa, Alexandre and Bovaird, Jenna and Brill, Leon and Broughton, Michael and Browne, David A. and Buchea, Brett and Buckley, Bob B. and Buell, David A. and Burger, Tim and Burkett, Brian and Bushnell, Nicholas and Cabrera, Anthony and Campero, Juan and Chang, Hung-Shen and Chen, Yu and Chen, Zijun and Chiaro, Ben and Chik, Desmond and Chou, Charina and Claes, Jahan and Cleland, Agnetta Y. and Cogan, Josh and Collins, Roberto and Conner, Paul and Courtney, William and Crook, Alexander L. and Curtin, Ben and Das, Sayan and Davies, Alex and Lorenzo, Laura De and Debroy, Dripto M. and Demura, Sean and Devoret, Michel and Paolo, Agustin Di and Donohoe, Paul and Drozdov, Ilya and Dunsworth, Andrew and Earle, Clint and Edlich, Thomas and Eickbusch, Alec and Elbag, Aviv Moshe and Elzouka, Mahmoud and Erickson, Catherine and Faoro, Lara and Farhi, Edward and Ferreira, Vinicius S. and Burgos, Leslie Flores and Forati, Ebrahim and Fowler, Austin G. and Foxen, Brooks and Ganjam, Suhas and Garcia, Gonzalo and Gasca, Robert and Genois, \'{E}lie and Giang, William and Gidney, Craig and Gilboa, Dar and Gosula, Raja and Dau, Alejandro Grajales and Graumann, Dietrich and Greene, Alex and Gross, Jonathan A. and Habegger, Steve and Hall, John and Hamilton, Michael C. and Hansen, Monica and Harrigan, Matthew P. and Harrington, Sean D. and Heras, Francisco J. H. and Heslin, Stephen and Heu, Paula and Higgott, Oscar and Hill, Gordon and Hilton, Jeremy and Holland, George and Hong, Sabrina and Huang, Hsin-Yuan and Huff, Ashley and Huggins, William J. and Ioffe, Lev B. and Isakov, Sergei V. and Iveland, Justin and Jeffrey, Evan and Jiang, Zhang and Jones, Cody and Jordan, Stephen and Joshi, Chaitali and Juhas, Pavol and Kafri, Dvir and Kang, Hui and Karamlou, Amir H. and Kechedzhi, Kostyantyn and Kelly, Julian and Khaire, Trupti and Khattar, Tanuj and Khezri, Mostafa and Kim, Seon and Klimov, Paul V. and Klots, Andrey R. and Kobrin, Bryce and Kohli, Pushmeet and Korotkov, Alexander N. and Kostritsa, Fedor and Kothari, Robin and Kozlovskii, Borislav and Kreikebaum, John Mark and Kurilovich, Vladislav D. and Lacroix, Nathan and Landhuis, David and Lange-Dei, Tiano and Langley, Brandon W. and Laptev, Pavel and Lau, Kim-Ming and Guevel, Lo\"{\i}ck Le and Ledford, Justin and Lee, Kenny and Lensky, Yuri D. and Leon, Shannon and Lester, Brian J. and Li, Wing Yan and Li, Yin and Lill, Alexander T. and Liu, Wayne and Livingston, William P. and Locharla, Aditya and Lucero, Erik and Lundahl, Daniel and Lunt, Aaron and Madhuk, Sid and Malone, Fionn D. and Maloney, Ashley and Mandr\'{a}, Salvatore and Martin, Leigh S. and Martin, Steven and Martin, Orion and Maxfield, Cameron and McClean, Jarrod R. and McEwen, Matt and Meeks, Seneca and Megrant, Anthony and Mi, Xiao and Miao, Kevin C. and Mieszala, Amanda and Molavi, Reza and Molina, Sebastian and Montazeri, Shirin and Morvan, Alexis and Movassagh, Ramis and Mruczkiewicz, Wojciech and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Nersisyan, Ani and Neven, Hartmut and Newman, Michael and Ng, Jiun How and Nguyen, Anthony and Nguyen, Murray and Ni, Chia-Hung and O'Brien, Thomas E. and Oliver, William D. and Opremcak, Alex and Ottosson, Kristoffer and Petukhov, Andre and Pizzuto, Alex and Platt, John and Potter, Rebecca and Pritchard, Orion and Pryadko, Leonid P. and Quintana, Chris and Ramachandran, Ganesh and Reagor, Matthew J. and Rhodes, David M. and Roberts, Gabrielle and Rosenberg, Eliott and Rosenfeld, Emma and Roushan, Pedram and Rubin, Nicholas C. and Saei, Negar and Sank, Daniel and Sankaragomathi, Kannan and Satzinger, Kevin J. and Schurkus, Henry F. and Schuster, Christopher and Senior, Andrew W. and Shearn, Michael J. and Shorter, Aaron and Shutty, Noah and Shvarts, Vladimir and Singh, Shraddha and Sivak, Volodymyr and Skruzny, Jindra and Small, Spencer and Smelyanskiy, Vadim and Smith, W. Clarke and Somma, Rolando D. and Springer, Sofia and Sterling, George and Strain, Doug and Suchard, Jordan and Szasz, Aaron and Sztein, Alex and Thor, Douglas and Torres, Alfredo and Torunbalci, M. Mert and Vaishnav, Abeer and Vargas, Justin and Vdovichev, Sergey and Vidal, Guifre and Villalonga, Benjamin and Heidweiller, Catherine Vollgraff and Waltman, Steven and Wang, Shannon X. and Ware, Brayden and Weber, Kate and White, Theodore and Wong, Kristi and Woo, Bryan W. K. and Xing, Cheng and Yao, Z. Jamie and Yeh, Ping and Ying, Bicheng and Yoo, Juhwan and Yosri, Noureldin and Young, Grayson and Zalcman, Adam and Zhang, Yaxing and Zhu, Ningfeng and Zobrist, Nicholas},
  year = {2024},
  month = aug,
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.13687},
  url = {http://arxiv.org/abs/2408.13687},
  urldate = {2024-12-04},
  note = {arXiv:2408.13687 [quant-ph]},
  abstract = {Quantum error correction provides a path to reach practical quantum computing by combining multiple physical qubits into a logical qubit, where the logical error rate is suppressed exponentially as more qubits are added. However, this exponential suppression only occurs if the physical error rate is below a critical threshold. In this work, we present two surface code memories operating below this threshold: a distance-7 code and a distance-5 code integrated with a real-time decoder. The logical error rate of our larger quantum memory is suppressed by a factor of \${\textbackslash}Lambda\$ = 2.14 \${\textbackslash}pm\$ 0.02 when increasing the code distance by two, culminating in a 101-qubit distance-7 code with 0.143\% \${\textbackslash}pm\$ 0.003\% error per cycle of error correction. This logical memory is also beyond break-even, exceeding its best physical qubit's lifetime by a factor of 2.4 \${\textbackslash}pm\$ 0.3. We maintain below-threshold performance when decoding in real time, achieving an average decoder latency of 63 \${\textbackslash}mu\$s at distance-5 up to a million cycles, with a cycle time of 1.1 \${\textbackslash}mu\$s. To probe the limits of our error-correction performance, we run repetition codes up to distance-29 and find that logical performance is limited by rare correlated error events occurring approximately once every hour, or 3 \${\textbackslash}times\$ 10\${\textasciicircum}9\$ cycles. Our results present device performance that, if scaled, could realize the operational requirements of large scale fault-tolerant quantum algorithms.},
  keywords = {Quantum Physics},
  annote = {Comment: 10 pages, 4 figures, Supplementary Information},
  file = {Preprint PDF:C\:\\Users\\alex\\Zotero\\storage\\SQ92UXPJ\\Acharya и др. - 2024 - Quantum error correction below the surface code threshold.pdf:application/pdf;Snapshot:C\:\\Users\\alex\\Zotero\\storage\\9YGIZ9SS\\2408.html:text/html},
}

@misc{QuantumErrorCorrection,
  title = {Quantum {Error} {Correction} {\textbar} {QuEra}},
  url = {https://www.quera.com/qec},
  urldate = {2024-12-04},
  abstract = {Using breakthrough technology, we are pleased to unveil a line of quantum computers with quantum error correction.},
  file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\5ZYT72VN\\qec.html:text/html},
}

@article{zhangGenerativeQuantumMachine2024,
  title = {Generative {Quantum} {Machine} {Learning} via {Denoising} {Diffusion} {Probabilistic} {Models}},
  author = {Zhang, Bingzhi and Xu, Peng and Chen, Xiaohui and Zhuang, Quntao},
  year = {2024},
  month = mar,
  journal = {Physical Review Letters},
  volume = {132},
  number = {10},
  pages = {100602},
  doi = {10.1103/PhysRevLett.132.100602},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.132.100602},
  urldate = {2024-12-04},
  note = {Publisher: American Physical Society},
  abstract = {Deep generative models are key-enabling technology to computer vision, text generation, and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and a relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the quantum denoising diffusion probabilistic model (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while it introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and demonstrate QuDDPM's capability in learning correlated quantum noise model, quantum many-body phases, and topological structure of quantum data. The results provide a paradigm for versatile and efficient quantum generative learning.},
  file = {Принятая версия:C\:\\Users\\alex\\Zotero\\storage\\H8U7A22J\\Zhang и др. - 2024 - Generative Quantum Machine Learning via Denoising Diffusion Probabilistic Models.pdf:application/pdf;APS Snapshot:C\:\\Users\\alex\\Zotero\\storage\\A8E7SM4C\\PhysRevLett.132.html:text/html},
}

@misc{kolleQuantumDenoisingDiffusion2024,
  title = {Quantum {Denoising} {Diffusion} {Models}},
  author = {K\"{o}lle, Michael and Stenzel, Gerhard and Stein, Jonas and Zielinski, Sebastian and Ommer, Bj\"{o}rn and Linnhoff-Popien, Claudia},
  year = {2024},
  month = jan,
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.07049},
  url = {http://arxiv.org/abs/2401.07049},
  urldate = {2024-12-04},
  note = {arXiv:2401.07049 [quant-ph]},
  abstract = {In recent years, machine learning models like DALL-E, Craiyon, and Stable Diffusion have gained significant attention for their ability to generate high-resolution images from concise descriptions. Concurrently, quantum computing is showing promising advances, especially with quantum machine learning which capitalizes on quantum mechanics to meet the increasing computational requirements of traditional machine learning algorithms. This paper explores the integration of quantum machine learning and variational quantum circuits to augment the efficacy of diffusion-based image generation models. Specifically, we address two challenges of classical diffusion models: their low sampling speed and the extensive parameter requirements. We introduce two quantum diffusion models and benchmark their capabilities against their classical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our models surpass the classical models with similar parameter counts in terms of performance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency model unitary single sampling architecture that combines the diffusion procedure into a single step, enabling a fast one-step image generation.},
  keywords = {Quantum Physics, Computer Science - Computer Vision and Pattern Recognition},
  file = {Preprint PDF:C\:\\Users\\alex\\Zotero\\storage\\G7AX4H8M\\Kölle и др. - 2024 - Quantum Denoising Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\alex\\Zotero\\storage\\C284MIIG\\2401.html:text/html},
}

@article{cherratQuantumVisionTransformers2024,
  title = {Quantum {Vision} {Transformers}},
  author = {Cherrat, El Amine and Kerenidis, Iordanis and Mathur, Natansh and Landman, Jonas and Strahm, Martin and Li, Yun Yvonna},
  year = {2024},
  month = feb,
  journal = {Quantum},
  volume = {8},
  pages = {1265},
  doi = {10.22331/q-2024-02-22-1265},
  url = {https://quantum-journal.org/papers/q-2024-02-22-1265/},
  urldate = {2024-12-04},
  note = {Publisher: Verein zur F\"{o}rderung des Open Access Publizierens in den Quantenwissenschaften},
  abstract = {El Amine Cherrat, Iordanis Kerenidis, Natansh Mathur, Jonas Landman, Martin Strahm, and Yun Yvonna Li, Quantum 8, 1265 (2024). In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in nat\ldots{}},
  file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\TKZEDIUN\\Cherrat и др. - 2024 - Quantum Vision Transformers.pdf:application/pdf},
}

@misc{khatriQuixerQuantumTransformer2024,
  title = {Quixer: {A} {Quantum} {Transformer} {Model}},
  shorttitle = {Quixer},
  author = {Khatri, Nikhil and Matos, Gabriel and Coopmans, Luuk and Clark, Stephen},
  year = {2024},
  month = jun,
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.04305},
  url = {http://arxiv.org/abs/2406.04305},
  urldate = {2024-12-04},
  note = {arXiv:2406.04305 [quant-ph]},
  abstract = {Progress in the realisation of reliable large-scale quantum computers has motivated research into the design of quantum machine learning models. We present Quixer: a novel quantum transformer model which utilises the Linear Combination of Unitaries and Quantum Singular Value Transform primitives as building blocks. Quixer operates by preparing a superposition of tokens and applying a trainable non-linear transformation to this mix. We present the first results for a quantum transformer model applied to a practical language modelling task, obtaining results competitive with an equivalent classical baseline. In addition, we include resource estimates for evaluating the model on quantum hardware, and provide an open-source implementation for classical simulation. We conclude by highlighting the generality of Quixer, showing that its parameterised components can be substituted with fixed structures to yield new classes of quantum transformers.},
  keywords = {Quantum Physics},
  annote = {Comment: 17 pages, 8 figures},
  file = {Preprint PDF:C\:\\Users\\alex\\Zotero\\storage\\TPD2VLRU\\Khatri и др. - 2024 - Quixer A Quantum Transformer Model.pdf:application/pdf;Snapshot:C\:\\Users\\alex\\Zotero\\storage\\Z7RAZ4JT\\2406.html:text/html},
}

@article{zhangTransformerQuantumState2023,
  title = {Transformer quantum state: {A} multipurpose model for quantum many-body problems},
  shorttitle = {Transformer quantum state},
  author = {Zhang, Yuan-Hang and Di Ventra, Massimiliano},
  year = {2023},
  month = feb,
  journal = {Physical Review B},
  volume = {107},
  number = {7},
  pages = {075147},
  doi = {10.1103/PhysRevB.107.075147},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.107.075147},
  urldate = {2024-12-04},
  note = {Publisher: American Physical Society},
  abstract = {Inspired by the advancements in large language models based on transformers, we introduce the transformer quantum state (TQS): a versatile machine learning model for quantum many-body problems. In sharp contrast to Hamiltonian/task specific models, TQS can generate the entire phase diagram, predict field strengths with experimental measurements, and transfer such a knowledge to new systems it has never been trained on before, all within a single model. With specific tasks, fine-tuning the TQS produces accurate results with small computational cost. Versatile by design, TQS can be easily adapted to new tasks, thereby pointing towards a general-purpose model for various challenging quantum problems.},
  file = {Отправленная версия:C\:\\Users\\alex\\Zotero\\storage\\4C6T77AX\\Zhang и Di Ventra - 2023 - Transformer quantum state A multipurpose model for quantum many-body problems.pdf:application/pdf;APS Snapshot:C\:\\Users\\alex\\Zotero\\storage\\A5NV2YHR\\PhysRevB.107.html:text/html},
}

@article{trugenbergerProbabilisticQuantumMemories2001,
  title = {Probabilistic {Quantum} {Memories}},
  author = {Trugenberger, C. A.},
  year = {2001},
  month = jul,
  journal = {Physical Review Letters},
  volume = {87},
  number = {6},
  pages = {067901},
  doi = {10.1103/PhysRevLett.87.067901},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.87.067901},
  urldate = {2024-12-04},
  note = {Publisher: American Physical Society},
  abstract = {Typical address-oriented computer memories cannot recognize incomplete or noisy information. Associative (content-addressable) memories solve this problem but suffer from severe capacity shortages. I propose a model of a quantum memory that solves both problems. The storage capacity is exponential in the number of qbits and thus optimal. The retrieval mechanism for incomplete or noisy inputs is probabilistic, with postselection of the measurement result. The output is determined by a probability distribution on the memory which is peaked around the stored patterns closest in Hamming distance to the input.},
  file = {Отправленная версия:C\:\\Users\\alex\\Zotero\\storage\\TKEW6M7B\\Trugenberger - 2001 - Probabilistic Quantum Memories.pdf:application/pdf;APS Snapshot:C\:\\Users\\alex\\Zotero\\storage\\CBRJU2RY\\PhysRevLett.87.html:text/html},
}

@misc{GPT4,
  title = {{GPT}-4},
  url = {https://openai.com/index/gpt-4/},
  urldate = {2024-12-05},
  abstract = {It can generate, edit, and iterate with users on creative and technical writing tasks, such as composing songs, writing screenplays, or learning a user's writing style.},
  file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\FTJ2E82Q\\gpt-4.html:text/html},
}

@misc{NobelPrizePhysicsa,
  title = {The {Nobel} {Prize} in {Physics}},
  year = {2022},
  journal = {NobelPrize.org},
  url = {https://www.nobelprize.org/prizes/physics/2022/summary/},
  urldate = {2024-12-05},
  abstract = {The Nobel Prize in Physics 2022 was awarded jointly to Alain Aspect, John F. Clauser and Anton Zeilinger "for experiments with entangled photons, establishing the violation of Bell inequalities and  pioneering quantum information science"},
  file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\AY6FIVLZ\\summary.html:text/html},
}

@misc{BreakthroughPrizeWinners,
  title = {Breakthrough {Prize} – {Winners} {Of} {The} 2023 {Breakthrough} {Prizes} {In} {Life} {Sciences}, {Mathematics} {And} {Fundamental} {Physics} {Announced}},
  url = {https://breakthroughprize.org/News/73},
  urldate = {2024-12-05},
  file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\PEC8FBJE\\73.html:text/html},
}

@article{nguyenBayesianQuantumNeural2022,
  title = {Bayesian {Quantum} {Neural} {Networks}},
  author = {Nguyen, Nam and Chen, Kwang-Cheng},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {54110--54122},
  doi = {10.1109/ACCESS.2022.3168675},
  issn = {2169-3536},
  url = {https://ieeexplore.ieee.org/document/9759399},
  urldate = {2024-12-05},
  note = {Conference Name: IEEE Access},
  abstract = {The astounding acceleration in Artificial Intelligence and Quantum Computing advances naturally gives rise to a line of research, which unrolls the potential advantages of quantum computing on classical Machine Learning tasks, known as Quantum Machine Learning or Quantum Machine Intelligence. The typical objectives are either (1) exploring the potential quantum advantages on classical learning tasks or (2) levering well-established classical ML algorithms to tackle quantum-related problems on Noisy Intermediate-Scale Quantum (NISQ) devices. Along the second research direction, we study Quantum Neural Networks (QNNs) to accomplish the purpose of Bayesian learning. By observing a wide range of studies on QNNs, in which the sole training method is based on frequentist training, we find that Bayesian learning benefits QNNs from two aspects. First, Bayesian-trained models enjoy a high level of generalization due to the prior and posterior distribution usage compared to frequentist training, which will be justified by this paper's theoretical study of model capacity. Second, Bayesian Inference offers epistemic uncertainty estimation, which merits the decision-making process. It is worth mentioning that frequentist-trained QNNs generally lack this desirable property. Under the Bayesian training procedure, our derived models can be considered a new class of QNNs (called BayesianQNNs) which possesses both desirable properties of Bayesian Inference while maintaining comparable predictive performance as frequentist counterparts. The proposed Bayesian Quantum Neural Networks is justified by empirical evidence from numerical experiments.},
  keywords = {Bayes methods, Bayesian inference, Bayesian learning, Data models, Integrated circuit modeling, Machine learning, Neural networks, Quantum computing, Quantum machine learning, quantum neural networks, Training},
  file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\JE9ECJU2\\Nguyen и Chen - 2022 - Bayesian Quantum Neural Networks.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\alex\\Zotero\\storage\\58MXZ99B\\9759399.html:text/html},
}

@article{daiQuantumBayesianOptimization2023,
  title = {Quantum {Bayesian} {Optimization}},
  author = {Dai, Zhongxiang and Lau, Gregory Kang Ruey and Verma, Arun and Shu, Yao and Low, Bryan Kian Hsiang and Jaillet, Patrick},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {20179--20207},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/401aa72e0e3be680348a5b0ffdb1a5aa-Abstract-Conference.html},
  urldate = {2024-12-05},
  file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\E7CGXQA9\\Dai и др. - 2023 - Quantum Bayesian Optimization.pdf:application/pdf},
}

@misc{QuantinuumsHSeriesHits,
  title = {Quantinuum's {H}-{Series} hits 56 physical qubits that are all-to-all connected, and departs the era of classical simulation},
  url = {https://www.quantinuum.com/blog/quantinuums-h-series-hits-56-physical-qubits-that-are-all-to-all-connected-and-departs-the-era-of-classical-simulation},
  urldate = {2024-12-08},
  abstract = {Quantinuum's H-Series combines full scalability with market-leading fidelity, performance, and error correction capabilities. In a demonstration with JPMorgan Chase \& Co., Quantinuum's H2-1 with 56 qubits achieved a massive uplift in an iconic demonstration.},
  file = {Snapshot:C\:\\Users\\alex\\Zotero\\storage\\3QEHQWUD\\quantinuums-h-series-hits-56-physical-qubits-that-are-all-to-all-connected-and-departs-the-era-.html:text/html},
}

@inproceedings{zhuangFastTrainingTripletBased2016,
  title = {Fast {Training} of {Triplet}-{Based} {Deep} {Binary} {Embedding} {Networks}},
  author = {Zhuang, Bohan and Lin, Guosheng and Shen, Chunhua and Reid, Ian},
  year = {2016},
  pages = {5955--5964},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Zhuang_Fast_Training_of_CVPR_2016_paper.html},
  urldate = {2024-12-08},
  file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\HKJN6EHD\\Zhuang и др. - 2016 - Fast Training of Triplet-Based Deep Binary Embedding Networks.pdf:application/pdf},
}

@inproceedings{yiBinaryEmbeddingFundamental2015,
  title = {Binary {Embedding}: {Fundamental} {Limits} and {Fast} {Algorithm}},
  shorttitle = {Binary {Embedding}},
  author = {Yi, Xinyang and Caramanis, Constantine and Price, Eric},
  year = {2015},
  month = jun,
  booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  pages = {2162--2170},
  url = {https://proceedings.mlr.press/v37/yi15.html},
  urldate = {2024-12-08},
  note = {ISSN: 1938-7228},
  abstract = {Binary embedding is a nonlinear dimension reduction methodology where high dimensional data are embedded into the Hamming cube while preserving the structure of the original space. Specifically, for an arbitrary N distinct points in {\textbackslash}mathbbS{\textasciicircum}p-1, our goal is to encode each point using m-dimensional binary strings such that we can reconstruct their geodesic distance up to \ensuremath{\delta}uniform distortion. Existing binary embedding algorithms either lack theoretical guarantees or suffer from running time O(mp). We make three contributions: (1) we establish a lower bound that shows any binary embedding oblivious to the set of points requires m =\ensuremath{\Omega}({\textbackslash}frac1\ensuremath{\delta}{\textasciicircum}2{\textbackslash}logN) bits and a similar lower bound for non-oblivious embeddings into Hamming distance; (2) we propose a novel fast binary embedding algorithm with provably optimal bit complexity m = O({\textbackslash}frac1 \ensuremath{\delta}{\textasciicircum}2{\textbackslash}logN) and near linear running time O(p {\textbackslash}log p) whenever {\textbackslash}log N \ll{}\ensuremath{\delta}{\textbackslash}sqrtp, with a slightly worse running time for larger {\textbackslash}log N; (3) we also provide an analytic result about embedding a general set of points K \subseteq{}{\textbackslash}mathbbS{\textasciicircum}p-1 with even infinite size. Our theoretical findings are supported through experiments on both synthetic and real data sets.},
  file = {Full Text PDF:C\:\\Users\\alex\\Zotero\\storage\\HZ5JFUPC\\Yi и др. - 2015 - Binary Embedding Fundamental Limits and Fast Algorithm.pdf:application/pdf},
}

@misc{AsymmetricDistancesBinary,
  title = {Asymmetric {Distances} for {Binary} {Embeddings} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
  url = {https://ieeexplore.ieee.org/abstract/document/6518116},
  urldate = {2024-12-08},
  file = {Asymmetric Distances for Binary Embeddings | IEEE Journals & Magazine | IEEE Xplore:C\:\\Users\\alex\\Zotero\\storage\\RB44G6N7\\6518116.html:text/html},
}
